{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 한국어 문서에 대한 BERT 활용\n",
    "\n",
    "다중 언어 BERT 사전학습 모형의 미세조정 학습\n",
    "\n",
    "BERT는 영어 이외에도 사전학습 모형을 지원한다.\n",
    "언어 모델로만 학습한 기본 학습 모형으로 bert-base-multilingual-uncased, bert-base-multilingual-cased가 있다.\n",
    "현재, bert-base-multilingual-cased가 더 많이 추천되고 있다.\n",
    "\n",
    "<한국어에 특화된 BERT 모형>\n",
    "1 . SKTBrain의 KoBERT\n",
    "2 . ETRI의 KorBERT (사용허가협약서가 필요)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set size : 8282\n",
      "#Validation set size : 2761\n",
      "#Test set size : 3682\n"
     ]
    }
   ],
   "source": [
    "#### 영화 리뷰를 읽기\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/ahyeo/OneDrive/문서/바탕 화면/Project2024/TextMining_ver02/data/daum_movie_review.csv\")\n",
    "#rating이 6보다 작으면 부정, 그 반대면 긍정\n",
    "y = [0 if rate < 6 else 1 for rate in df.rating]\n",
    "#데이터셋 분리\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(df.review.tolist(), y, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state = 0)\n",
    "\n",
    "print('#Train set size :', len(X_train))\n",
    "print('#Validation set size :', len(X_val))\n",
    "print('#Test set size :', len(X_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CommitInfo' from 'huggingface_hub' (C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\huggingface_hub\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m##미세조정을 위해 앞 장에서 배운 Trainer을 사용한다\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_metric\n\u001B[0;32m      5\u001B[0m metric \u001B[38;5;241m=\u001B[39m load_metric(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;66;03m#accuracy metric load\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# 모델 평가시 사용하는 함수\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\datasets\\__init__.py:18\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# ruff: noqa\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[0;32m     16\u001B[0m __version__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2.17.1\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marrow_dataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marrow_reader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ReadInstruction\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbuilder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\datasets\\arrow_dataset.py:62\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyarrow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpa\u001B[39;00m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyarrow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompute\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpc\u001B[39;00m\n\u001B[1;32m---> 62\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhuggingface_hub\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CommitInfo, CommitOperationAdd, CommitOperationDelete, DatasetCard, DatasetCardData, HfApi\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmultiprocess\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Pool\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcontrib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconcurrent\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m thread_map\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'CommitInfo' from 'huggingface_hub' (C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\huggingface_hub\\__init__.py)"
     ]
    }
   ],
   "source": [
    "##미세조정을 위해 앞 장에서 배운 Trainer을 사용한다\n",
    "import torch\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\") #accuracy metric load\n",
    "\n",
    "# 모델 평가시 사용하는 함수\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred # 모델 logits, 실제 레이블\n",
    "    predictions = np.argmax(logits, axis = -1) # 모델의 예측값 계산\n",
    "    return metric.compute(predictions = predictions, references = labels) # 실제 label과 비교해 모델 평가\n",
    "\n",
    "#데이터셋 정의 -> kobert 설치로 인한 의존성 충돌 ..\n",
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, labels) :\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "#idx에 해당하는 데이터 샘플을 가져오는 매서드\n",
    "    def __getitem__(self, idx):\n",
    "        # 모델에 입력될 데이터를 tensor 형태로 변환, 해당 샘플의 label을 포함한 딕셔너리 반환\n",
    "        item = {key:torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 9521, 118741, 35506, 24982, 48549, 117, 9321, 118611, 119081, 48345, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 9521, 118741, 35506, 24982, 48549, 117, 9321, 118610, 119081, 48345, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "print(tokenizer(\"안녕하세요, 반값습니다.\"))\n",
    "inputs = tokenizer(\"안녕하세요, 반갑습니다.\")\n",
    "print(inputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 검증 관련 매개변수 정의\n",
    "\n",
    "Trainer\n",
    "\n",
    "TrainingArguments\n",
    "per_device_eval_batch_size : 하이퍼파라미터를 정의, batch size 조절\n",
    "evaluation_strategy : steps -> eval_steps = 500, 검증을 매 500 스텝마다 실시한다.\n",
    "step은 각 배치에 대한 학습을 의미하며 epoch 단위로 하고 싶다면, evaluation_strategy 값을 epochs로 준다.\n",
    "learning_rate_scheduler : 경사하강법에서 경사가 낮은 쪽으로 얼마나 멀리 갈 것인지 결정하는 하이퍼파라미터\n",
    "학습률이 크면, 학습속도는 빨라지지만 수렴을 놓칠 수 있다. 반대로 학습률이 낮으면, 미세하게 움직여 수렴은 잘 되지만 오래 걸린다.\n",
    "-> 그러므로 처음에는 학습률을 크게, 갈수록 학습률을 낮게 설정하는 것이 좋다.\n",
    "get_linear_schedule_with_warmup : 스케줄러에 대한 기본값 설정\n",
    "warmup : 학습을 낮은 학습률로 몸을 풀어주고 시작하면 학습 효과가 올라간다. (0 -> 1까지 수행하고 점차 내려가는 방식)\n",
    "weight_decay : 가중치가 너무 커지지 않게 패널티를 주어 과적합을 방지하는 도구 (값이 작을 수록 패널티를 약하게 주는 것)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "#토큰화\n",
    "#truncation :  입력 시퀀스의 길이가 모델이 처리할 수 있는 최대 길이를 초과할 경우, 초과하는 부분을 자르고(즉, 잘라내고) 최대 길이에 맞춘다.\n",
    "train_input = tokenizer(X_train, truncation = True, padding = True, return_tensors='pt')\n",
    "val_input = tokenizer(X_val, truncation = True, padding = True, return_tensors='pt')\n",
    "test_input = tokenizer(X_test, truncation = True, padding = True, return_tensors='pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#데이터셋 생성\n",
    "train_dataset = OurDataset(train_input, y_train)\n",
    "val_dataset = OurDataset(val_input, y_train)\n",
    "test_dataset = OurDataset(test_input, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "#Trainer에 사용할 파라미터 지정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    num_train_epochs=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps = 5,\n",
    "    per_device_train_batch_size= 8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=200, #warmup steps 수\n",
    "    weight_decay=0.01\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "##trainer 객체 설정\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset, #검증 데이터셋\n",
    "    compute_metrics= compute_metrics\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahyeo\\AppData\\Local\\Temp\\ipykernel_16904\\1094331634.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key:torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='2072' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/2072 : < :, Epoch 0.00/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\ahyeo\\AppData\\Local\\Temp\\ipykernel_16904\\49973641.py\", line 1, in <module>\n",
      "    trainer.train()\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\transformers\\trainer.py\", line 1624, in train\n",
      "    method :meth:`~transformers.Trainer.create_optimizer_and_scheduler` for custom optimizer/scheduler.\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\transformers\\trainer.py\", line 1961, in _inner_training_loop\n",
      "    return\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\transformers\\trainer.py\", line 2902, in training_step\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\transformers\\trainer.py\", line 2925, in compute_loss\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\n",
      "    for name, param in self.named_parameters(recurse=recurse):\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 1564, in forward\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\n",
      "    for name, param in self.named_parameters(recurse=recurse):\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 1013, in forward\n",
      "    hidden_states=encoder_outputs.hidden_states,\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\n",
      "    for name, param in self.named_parameters(recurse=recurse):\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 607, in forward\n",
      "    hidden_states,\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\n",
      "    for name, param in self.named_parameters(recurse=recurse):\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 497, in forward\n",
      "    head_mask,\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\n",
      "    for name, param in self.named_parameters(recurse=recurse):\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 427, in forward\n",
      "    return hidden_states\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\n",
      "    for name, param in self.named_parameters(recurse=recurse):\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 359, in forward\n",
      "    def forward(self, hidden_states, input_tensor):\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\n",
      "    for name, param in self.named_parameters(recurse=recurse):\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\torch\\nn\\modules\\dropout.py\", line 59, in forward\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\torch\\nn\\functional.py\", line 1268, in dropout\n",
      "    else:\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1055, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 955, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 778, in lines\n",
      "    return self._sd.lines\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\executing\\executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# trainer.save_model(\"my_model\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##성능 측정\n",
    "trainer.evaluate(eval_dataset = test_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 분석 결과\n",
    "\n",
    "**GPU가 없어서 오래 걸리기 때문에 .. 책에 나와있는 출력값을 참고하였다.**\n",
    "\n",
    "학습 데이터셋과 검증 데이터셋에 대한 손실이 모두 꾸준히 줄어든다. (train loss값 0.5 → 0.44까지 줄어듦, val loss도 비슷하게 줄어듦)\n",
    "학습이 정상적으로 잘 진행되었다고 볼 수 있다.\n",
    "학습을 조금 더 진행했다면, 더 좋은 성능을 보였을 것 같다. (현재 정확도 : 81.9%)\n",
    "\n",
    "Trainer는 검증 평가 스텝에서 자동으로 학습 중인 모형을 저장한다.\n",
    "저장을 할 땐 save_model()를, 불러오고 싶을 땐 from_pretrained() method를 사용하면 된다.\n",
    "\n",
    "test_dataset으로 성능을 측정하였을 때도 80.1% 정도의 높은 성능을 보여준다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### KoBERT 사전학습 모형에 대한 파이토치 미세조정\n",
    "\n",
    "bert 모델이 한국어 문서에 특화되어 있는 모델이 아니고 토크나이저 역시 한국어 구조에 맞게 만들어진 것이 아니기 때문에\n",
    "한국어에 대해 좋은 성능을 기대하기 어렵다.\n",
    "\n",
    "KoBERT는 한국어 위키 기반으로 학습해 만든 사전학습 모형으로 다른 모델보다는 더 좋은 성능을 보인다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "#이전 캐시 및 모델 지우기\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 371k/371k [00:00<00:00, 1.10MB/s]\n",
      "Downloading: 100%|██████████| 244/244 [00:00<?, ?B/s] \n",
      "Downloading: 100%|██████████| 432/432 [00:00<?, ?B/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁안', '녕', '하세요', '.', '▁반', '갑', '습니다', '.']\n",
      "{'input_ids': [2, 3135, 5724, 7814, 54, 2207, 5345, 6701, 54, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "#koBERT의 tokenizer 불러오기\n",
    "#colab을 사용하는 것이 좋을 것 같습니다..\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "\n",
    "print(tokenizer.tokenize(\"안녕하세요. 반갑습니다.\"))\n",
    "inputs = tokenizer(\"안녕하세요. 반갑습니다.\")\n",
    "print(inputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#데이터셋 정의\n",
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, labels) :\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "#idx에 해당하는 데이터 샘플을 가져오는 매서드\n",
    "    def __getitem__(self, idx):\n",
    "        # 모델에 입력될 데이터를 tensor 형태로 변환, 해당 샘플의 label을 포함한 딕셔너리 반환\n",
    "        item = {key:torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_input = tokenizer(X_train, truncation=True, padding = True, return_tensors= 'pt')\n",
    "val_input = tokenizer(X_val, truncation=True, padding = True, return_tensors= 'pt')\n",
    "test_input = tokenizer(X_test, truncation=True, padding = True, return_tensors= 'pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "train_dataset = OurDataset(train_input, y_train)\n",
    "val_dataset = OurDataset(val_input, y_val)\n",
    "test_dataset = OurDataset(test_input, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, shuffle = True, batch_size = 8)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 16)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 16)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 535/535 [00:00<00:00, 268kB/s]\n",
      "Downloading: 100%|██████████| 369M/369M [01:15<00:00, 4.88MB/s] \n"
     ]
    }
   ],
   "source": [
    "#kobert 로드\n",
    "bert_model = BertModel.from_pretrained('skt/kobert-base-v1')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, pretrained_model, token_size, num_labels):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.token_size = token_size\n",
    "        self.num_labels = num_labels\n",
    "        self.pretrained_model = pretrained_model\n",
    "\n",
    "        #분류기 정의\n",
    "        self.classifier = torch.nn.Linear(self.token_size, self.num_labels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.pretrained_model(**inputs)\n",
    "        # BERT 출력에서 CLS 토큰에 해당하는 부분만 가져온다.\n",
    "        bert_clf_token = outputs.last_hidden_state[:,0,:]\n",
    "\n",
    "        return self.classifier(bert_clf_token)\n",
    "\n",
    "model = MyModel(bert_model, num_labels = 2, token_size=bert_model.config.hidden_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahyeo\\AppData\\Local\\Temp\\ipykernel_35620\\3313682083.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key:torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 40\u001B[0m\n\u001B[0;32m     38\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, F\u001B[38;5;241m.\u001B[39mone_hot(labels, num_classes \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mfloat())  \n\u001B[0;32m     39\u001B[0m train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\n\u001B[1;32m---> 40\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     41\u001B[0m optim\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     42\u001B[0m scheduler\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\torch\\_tensor.py:307\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    298\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    299\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    300\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    301\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    305\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[0;32m    306\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[1;32m--> 307\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\torch\\autograd\\__init__.py:154\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retain_graph \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    152\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m--> 154\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    155\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    156\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "#GPU 설정\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# optimizer 설정\n",
    "optim = AdamW(model.parameters(), lr = 5e-5, weight_decay=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss() #손실함수\n",
    "\n",
    "num_epochs = 2\n",
    "total_training_steps = num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer=optim,\n",
    "                                            num_training_steps=total_training_steps,\n",
    "                                            num_warmup_steps=200)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "train_loss = 0\n",
    "eval_steps = 500\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        model.train()\n",
    "        optim.zero_grad() #gradient 초기화\n",
    "\n",
    "        inputs = {k:v.to(device) for k, v in batch.items() if k!='labels'} #배치에서 label을 제외한 입력만 추출해 GPU로 복사\n",
    "        labels = batch['labels'].to(device) #batch에서 label 추출\n",
    "        outputs = model(inputs) #모형 예측\n",
    "\n",
    "        #손실함수를 통해 두 클래스에 대해 예측하고 각각 비교\n",
    "        #labels에 one-hot-encoding 적용\n",
    "        loss = criterion(outputs, F.one_hot(labels, num_classes = 2).float())\n",
    "        train_loss += loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        step+=1\n",
    "        #검증 시간 -> train loss, validation loss 출력\n",
    "        if step % eval_steps == 0:\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                model.eval()\n",
    "                for batch in val_loader:\n",
    "                    inputs = {k:v.to(device) for k, v in batch.items() if k!='labels'} #배치에서 label을 제외한 입력만 추출해 GPU로 복사\n",
    "                    labels = batch['labels'].to(device) #batch에서 label 추출\n",
    "                    outputs = model(inputs) #모형 예측\n",
    "                    loss = criterion(outputs, F.one_hot(labels, num_classes = 2).float())\n",
    "                    val_loss += loss\n",
    "                avg_val_loss = val_loss/len(val_loader)\n",
    "\n",
    "            avg_train_loss =train_loss/eval_steps\n",
    "            elapsed = time.time() - start\n",
    "\n",
    "            print(\n",
    "                \"Step %d, elapsed time: %.2f, train loss : %.4f, validation loss: %.4f\"\n",
    "                % (step, elapsed, avg_train_loss, avg_val_loss)\n",
    "            )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CommitInfo' from 'huggingface_hub' (C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\huggingface_hub\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_metric\n\u001B[0;32m      2\u001B[0m metric \u001B[38;5;241m=\u001B[39m load_metric(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\datasets\\__init__.py:18\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# ruff: noqa\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[0;32m     16\u001B[0m __version__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2.17.1\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marrow_dataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marrow_reader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ReadInstruction\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbuilder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\datasets\\arrow_dataset.py:62\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyarrow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpa\u001B[39;00m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyarrow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompute\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpc\u001B[39;00m\n\u001B[1;32m---> 62\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhuggingface_hub\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CommitInfo, CommitOperationAdd, CommitOperationDelete, DatasetCard, DatasetCardData, HfApi\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmultiprocess\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Pool\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcontrib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconcurrent\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m thread_map\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'CommitInfo' from 'huggingface_hub' (C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining_ver02\\lib\\site-packages\\huggingface_hub\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    inputs = {k:v.to(device) for k, v in batch.items() if k!='labels'}\n",
    "    labels = batch['labels'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "\n",
    "    predictions = torch.argmax(outputs, dim = -1)\n",
    "    metric.add_batch(predictions = predictions, references = labels)\n",
    "\n",
    "metric.compute()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 결과 분석\n",
    "\n",
    "GPU가 없고 hugging face의 datasets 의존성 문제로 수행 불가\n",
    "\n",
    "train을 수행하면서 train loss와 validation loss 모두 줄어드는 것을 볼 수 있음\n",
    "test에 대한 accuracy도 86.90%로 확연히 뛰어난 성능을 보였음"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
