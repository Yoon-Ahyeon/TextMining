{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Bow 기반의 문서 분류\n",
    "\n",
    "주어진 문서를 미리 정의된 클래스로 분류하는 작업\n",
    "\n",
    "<머신러닝의 분류 작업>\n",
    "\n",
    "1. 로지스틱 회귀분석\n",
    "2. 결정트리\n",
    "3. 나이브 베이즈 **(가장 많이 사용)**\n",
    "4. SVM\n",
    "\n",
    "※ 주의사항\n",
    "\n",
    "뉴스의 분야가 라벨로 달려 있어야 한다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set size :  2034\n",
      "#Test set size :  1353\n",
      "#Selected categories :  ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
      "Train labels :  {0, 1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "newsgroup_train = fetch_20newsgroups(subset = 'train', remove = ('headers', 'footers',\n",
    "                                                                 'quotes'),\n",
    "                                     categories = categories)\n",
    "newsgroup_test = fetch_20newsgroups(subset = 'test', remove = ('headers', 'footers',\n",
    "                                                                 'quotes'),\n",
    "                                     categories = categories)\n",
    "\n",
    "print('#Train set size : ', len(newsgroup_train.data))\n",
    "print('#Test set size : ', len(newsgroup_test.data))\n",
    "print('#Selected categories : ', newsgroup_train.target_names)\n",
    "print('Train labels : ', set(newsgroup_train.target))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set text samples :  Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "#Train set label samples :  1\n",
      "#Test set text samples :  TRry the SKywatch project in  Arizona.\n",
      "#Test set label samples :  2\n"
     ]
    }
   ],
   "source": [
    "print(\"#Train set text samples : \", newsgroup_train.data[0])\n",
    "print(\"#Train set label samples : \", newsgroup_train.target[0])\n",
    "print(\"#Test set text samples : \", newsgroup_test.data[0])\n",
    "print(\"#Test set label samples : \", newsgroup_test.target[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 카운트 기반 특성 추출"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set dimension :  (2034, 2000)\n",
      "Test set dimension (1353, 2000)\n"
     ]
    }
   ],
   "source": [
    "X_train = newsgroup_train.data\n",
    "y_train = newsgroup_train.target\n",
    "\n",
    "X_test = newsgroup_test.data\n",
    "y_test = newsgroup_test.target\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# min_df : 단어가 최소 이 문서에 5개는 나타나야 한다. (문서에서 거의 쓰이지 않는다 - 예외)\n",
    "# max_df : 단어가 문서의 50%를 초과해 나타나는 단어를 제외한다.\n",
    "cv = CountVectorizer(max_features = 2000, min_df = 5, max_df = 0.5)\n",
    "\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "print('Train set dimension : ', X_train_cv.shape) # (train 문서의 수, 특성의 개수)\n",
    "X_test_cv = cv.transform(X_test)\n",
    "print('Test set dimension', X_test_cv.shape) # (test 문서의 수, 특성의 개수)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 : 0, 000 : 0, 01 : 0, 04 : 0, 05 : 0, 10 : 0, 100 : 0, 1000 : 0, 11 : 0, 12 : 0, 128 : 0, 129 : 0, 13 : 0, 130 : 0, 14 : 0, 15 : 0, 16 : 0, 17 : 0, 18 : 0, 19 : 0, 1987 : 0, 1988 : 0, 1989 : 0, 1990 : 0, 1991 : 0, 1992 : 0, 1993 : 0, 20 : 0, 200 : 0, 202 : 0, 21 : 0, 22 : 0, 23 : 0, 24 : 0, 25 : 0, 256 : 0, 26 : 0, 27 : 0, 28 : 0, 2d : 0, 30 : 0, 300 : 0, 31 : 0, 32 : 0, 33 : 0, 34 : 0, 35 : 0, 39 : 0, 3d : 0, 40 : 0, 400 : 0, 42 : 0, 45 : 0, 50 : 0, 500 : 0, 60 : 0, 600 : 0, 65 : 0, 70 : 0, 75 : 0, 80 : 0, 800 : 0, 90 : 0, 900 : 0, 91 : 0, 92 : 0, 93 : 0, 95 : 0, _the : 0, ability : 0, able : 1, abortion : 0, about : 1, above : 0, absolute : 0, absolutely : 0, ac : 0, accept : 0, acceptable : 0, accepted : 0, access : 0, according : 0, account : 0, accurate : 0, across : 0, act : 0, action : 0, actions : 0, active : 0, activities : 0, activity : 0, acts : 0, actual : 0, actually : 0, ad : 0, add : 0, added : 0, addition : 0, additional : 0, address : 0, "
     ]
    }
   ],
   "source": [
    "for word, count in zip(\n",
    "    # 사용된 단어의 이름과 빈도 수 추출\n",
    "    cv.get_feature_names_out()[:100], X_train_cv[0].toarray()[0, :100]\n",
    "):\n",
    "    print(word, ':', count, end = ', ')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. 나이브베이즈를 이용한 문서 분류\n",
    "\n",
    "새로운 기사에 나온 단어들이 각각 어떤 확률로 경제 관련 기사와 과학 기사에 나오는지 계산하고, 이를 잘 결합하여\n",
    "**새로운 기사가 어느 분야**에 속하는 지 확인한다.\n",
    "\n",
    "사전확률을 계산하여, 특성을 고려해 이를 더 정확한 확률로 바꾼다.\n",
    "\n",
    "Q. MultinomialNB\n",
    "이산적인 특성 값들을 이용해 분류하고자 할 때 사용한다.\n",
    "이산적이란, 연속적인 값이 아닌 값이라는 뜻으로 countvector가 이에 해당한다.\n",
    "\n",
    "매개변수\n",
    "alpha : 모델의 복잡도 조절 (이 값을 늘리면, 통계 데이터가 완만해지고 복잡도가 낮아진다.)\n",
    "TFidfVectorizer 사용 가능"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score : 0.824\n",
      "Test set score : 0.732\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "NB_clf = MultinomialNB()\n",
    "NB_clf.fit(X_train_cv, y_train)\n",
    "\n",
    "print('Train set score : {:.3f}'.format(NB_clf.score(X_train_cv, y_train)))\n",
    "print('Test set score : {:.3f}'.format(NB_clf.score(X_test_cv, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#First document and label in test data :  TRry the SKywatch project in  Arizona. 2\n",
      "#second document and label in test data :  The Vatican library recently made a tour of the US.\n",
      " Can anyone help me in finding a FTP site where this collection is \n",
      " available. 1\n",
      "#Predicted Labels :  [2 1]\n",
      "#Predicted categories :  sci.space comp.graphics\n"
     ]
    }
   ],
   "source": [
    "print('#First document and label in test data : ', X_test[0], y_test[0])\n",
    "print('#second document and label in test data : ', X_test[1], y_test[1])\n",
    "\n",
    "pred = NB_clf.predict(X_test_cv[:2])\n",
    "\n",
    "print('#Predicted Labels : ', pred)\n",
    "\n",
    "# 주제 (topic) 전달\n",
    "print(\n",
    "    '#Predicted categories : ',\n",
    "    newsgroup_train.target_names[pred[0]],\n",
    "    newsgroup_train.target_names[pred[1]]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score : 0.862\n",
      "Test set score : 0.741\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=2000, min_df = 5, max_df = 0.5)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "NB_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('Train set score : {:.3f}'. format(NB_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score : {:.3f}'. format(NB_clf.score(X_test_tfidf, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(NB_clf, 'coef_')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism : you,not,are,be,this,have,as,what,they,if\n",
      "comp.graphics : you,on,graphics,this,have,any,can,or,with,thanks\n",
      "sci.space : space,on,you,be,was,this,as,they,have,are\n",
      "talk.religion.misc : you,not,he,are,as,this,be,god,was,they\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def top10_features(classifier, vectorizer, categories):\n",
    "    # word 이름 추출\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names_out())\n",
    "    for i, category in enumerate(categories):\n",
    "        # 역순 정렬을 위해 계수에 음수를 취해 정렬 후, 앞에서 10개 추출\n",
    "        top10 = np.argsort(-classifier.feature_log_prob_[i])[:10]\n",
    "        # 카테고리와 영향이 큰 특성 10개 반환\n",
    "        print(\"%s : %s\" % (category, \",\".join(feature_names[top10])))\n",
    "\n",
    "top10_features(NB_clf, tfidf, newsgroup_train.target_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 로지스틱 회귀분석을 이용한 문서 분류\n",
    "\n",
    "회귀분석은 예측하고자 하는 값이 연속적일 때 사용하는 반면, 로지스틱 회귀 분석은 라벨이 연속적인 값이 아니고 분류에 해당될 때 사용한다.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score : 0.930\n",
      "Test set score : 0.734\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR_clf = LogisticRegression()\n",
    "\n",
    "LR_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('Train set score : {:.3f}'. format(LR_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score : {:.3f}'. format(LR_clf.score(X_test_tfidf, y_test))) # 과적합 발생 가능성 있음"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 과적합 해결 방법\n",
    "\n",
    "※ 릿지 회귀를 이용한 과적합 방지\n",
    "\n",
    "릿지 회귀는 회귀분석에 정규화를 사용하는 알고리즘으로, 최적화를 위한 목적함수에 정규화 항목을 넣어서 특성에 대한 계수가 지나치게 커지는 것을 억제한다.\n",
    "\n",
    "Ridge Classfier의 매개변수\n",
    "alpha : 정규화의 정도 조절 (값이 커질 수록, 정규화의 비중이 커져 계수를 더 많이 억제한다.)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score : 0.960\n",
      "Test set score : 0.735\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "ridge_clf = RidgeClassifier()\n",
    "\n",
    "ridge_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('Train set score : {:.3f}'. format(ridge_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score : {:.3f}'. format(ridge_clf.score(X_test_tfidf, y_test))) # 과적합 발생 가능성 있음"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 과적합 해결방법\n",
    "\n",
    "1 . validation set을 이용한 그리드 서치\n",
    "2 . 하이퍼파라미터 튜닝"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Alpha 1.600000 at max validation score 0.825553\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_ridge, X_val_ridge, y_train_ridge, y_val_ridge = train_test_split(X_train_tfidf, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "max_score = 0\n",
    "max_alpha = 0\n",
    "\n",
    "for alpha in np.arange(0.1, 10, 0.1):\n",
    "    ridge_clf = RidgeClassifier(alpha = alpha)\n",
    "    ridge_clf.fit(X_train_ridge, y_train_ridge)\n",
    "    score = ridge_clf.score(X_val_ridge, y_val_ridge)\n",
    "    if score > max_score :\n",
    "        max_score = score\n",
    "        max_alpha = alpha\n",
    "\n",
    "print('Max Alpha {:3f} at max validation score {:3f}'.format(max_alpha, max_score))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score : 0.947\n",
      "Test set score : 0.739\n"
     ]
    }
   ],
   "source": [
    "ridge_clf = RidgeClassifier(alpha = 1.6)\n",
    "ridge_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('Train set score : {:.3f}'. format(ridge_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score : {:.3f}'. format(ridge_clf.score(X_test_tfidf, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def top10_features(classifier, vectorizer, categories):\n",
    "    # word 이름 추출\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names_out())\n",
    "    for i, category in enumerate(categories):\n",
    "        # 역순 정렬을 위해 계수에 음수를 취해 정렬 후, 앞에서 10개 추출\n",
    "        top10 = np.argsort(-classifier.coef_[i])[:10]\n",
    "        # 카테고리와 영향이 큰 특성 10개 반환\n",
    "        print(\"%s : %s\" % (category, \",\".join(feature_names[top10])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism : bobby,religion,atheism,atheists,motto,punishment,islam,deletion,islamic,satan\n",
      "comp.graphics : graphics,computer,3d,file,image,hi,42,using,screen,looking\n",
      "sci.space : space,orbit,nasa,spacecraft,moon,sci,launch,flight,funding,idea\n",
      "talk.religion.misc : christian,christians,fbi,blood,order,jesus,objective,children,christ,hudson\n"
     ]
    }
   ],
   "source": [
    "top10_features(ridge_clf, tfidf, newsgroup_train.target_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 라쏘 회귀를 이용한 특성 선택\n",
    "\n",
    "특성의 계수에 대해 정규화를 한다.\n",
    "라쏘는 정규화를 할 때, 특성의 계수가 0에 가까워지면, 이를 완전히 0으로 바꾼다. (절댓값)\n",
    "어떤 특성의 계수가 0이라는 것은 그 특성은 분류에 전혀 영향을 미치지 않는다는 것이다.\n",
    "-> 특성의 수를 줄여주는 효과가 있다. (정확도가 정확히 향상된다고 보기는 어렵다.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score : 0.819\n",
      "Test set score : 0.724\n"
     ]
    }
   ],
   "source": [
    "lasso_clf = LogisticRegression(penalty='l1', solver = 'liblinear', C = 1) # C는 alpha의 역수\n",
    "\n",
    "lasso_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('Train set score : {:.3f}'. format(lasso_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score : {:.3f}'. format(lasso_clf.score(X_test_tfidf, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Used features count : 437 out of 2000\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    '#Used features count : {}'.format(np.sum(lasso_clf.coef_ != 0)), #특성 선택\n",
    "    'out of',\n",
    "    X_train_tfidf.shape[1]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism : bobby,atheism,atheists,islam,religion,islamic,motto,atheist,satan,vice\n",
      "comp.graphics : graphics,image,3d,file,computer,hi,video,files,looking,sphere\n",
      "sci.space : space,orbit,launch,nasa,spacecraft,flight,moon,dc,shuttle,solar\n",
      "talk.religion.misc : fbi,christian,christians,christ,order,jesus,children,objective,context,blood\n"
     ]
    }
   ],
   "source": [
    "top10_features(lasso_clf, tfidf, newsgroup_train.target_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 결정트리를 이용한 문서 분류 방법\n",
    "\n",
    "1. DecisionTree\n",
    "2. RandomForest\n",
    "3. GradientBoosting\n",
    "\n",
    "결정트리가 과적합되는 성향이 매우 강하다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score : 0.977 Test set score : 0.536\n",
      "Train set score : 0.977 Test set score : 0.685\n",
      "Train set score : 0.933 Test set score : 0.696\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=7)\n",
    "tree.fit(X_train_tfidf, y_train)\n",
    "print(\n",
    "    'Train set score : {:.3f}'. format(tree.score(X_train_tfidf, y_train)),\n",
    "    'Test set score : {:.3f}'. format(tree.score(X_test_tfidf, y_test))\n",
    ")\n",
    "\n",
    "forest = RandomForestClassifier(random_state=7)\n",
    "forest.fit(X_train_tfidf, y_train)\n",
    "print(\n",
    "    'Train set score : {:.3f}'. format(forest.score(X_train_tfidf, y_train)),\n",
    "    'Test set score : {:.3f}'. format(forest.score(X_test_tfidf, y_test))\n",
    ")\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=7)\n",
    "gb.fit(X_train_tfidf, y_train)\n",
    "print(\n",
    "    'Train set score : {:.3f}'. format(gb.score(X_train_tfidf, y_train)),\n",
    "    'Test set score : {:.3f}'. format(gb.score(X_test_tfidf, y_test))\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "space : 0.126, graphics : 0.080, atheism : 0.024, thanks : 0.023, file : 0.021, orbit : 0.020, jesus : 0.018, god : 0.018, hi : 0.017, nasa : 0.015, image : 0.015, files : 0.014, christ : 0.010, moon : 0.010, bobby : 0.010, launch : 0.010, looking : 0.010, christian : 0.010, atheists : 0.009, christians : 0.009, fbi : 0.009, 3d : 0.008, you : 0.008, not : 0.008, islamic : 0.007, religion : 0.007, spacecraft : 0.007, flight : 0.007, computer : 0.007, islam : 0.007, ftp : 0.006, color : 0.006, software : 0.005, atheist : 0.005, card : 0.005, people : 0.005, koresh : 0.005, his : 0.005, kent : 0.004, sphere : 0.004, "
     ]
    }
   ],
   "source": [
    "sorted_feature_importances = sorted(\n",
    "    zip(tfidf.get_feature_names_out(), gb.feature_importances_),\n",
    "    key = lambda x:x[1],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "for feature, value in sorted_feature_importances[:40]:\n",
    "    print('%s : %.3f' % (feature, value), end = ', ')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%ㅇ\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 성능 높이기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score : 0.930\n",
      "Test set score : 0.751\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words('english')\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "\n",
    "RegTok = RegexpTokenizer(\"[\\w']{3,}\")\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = RegTok.tokenize(text.lower())\n",
    "    words = [word for word in tokens if (word not in english_stops) and len(word) > 2]\n",
    "    features = (list(map(lambda token: PorterStemmer().stem(token), words)))\n",
    "    return features\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer = tokenizer, max_features= 2000, max_df = 0.5, min_df = 5)\n",
    "\n",
    "X_train_tfidf= tfidf.fit_transform(X_train)\n",
    "X_test_tfidf=  tfidf.transform(X_test)\n",
    "\n",
    "LR_clf = LogisticRegression()\n",
    "LR_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('Train set score : {:.3f}'. format(LR_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score : {:.3f}'. format(LR_clf.score(X_test_tfidf, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set Dimension :  (2034, 4056)\n",
      "Train set score : 0.953\n",
      "Test set score : 0.761\n"
     ]
    }
   ],
   "source": [
    "# 특성 추출기의 단어 제한을 없앤다\n",
    "tfidf = TfidfVectorizer(tokenizer = tokenizer, max_df = 0.5, min_df = 5)\n",
    "\n",
    "X_train_tfidf= tfidf.fit_transform(X_train)\n",
    "X_test_tfidf=  tfidf.transform(X_test)\n",
    "\n",
    "LR_clf = LogisticRegression()\n",
    "LR_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"#Train set Dimension : \", X_train_tfidf.shape)\n",
    "print('Train set score : {:.3f}'. format(LR_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score : {:.3f}'. format(LR_clf.score(X_test_tfidf, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 카운트 기반의 문제점과 N-gram을 이용한 보완\n",
    "\n",
    "Bow 같은 경우, 단어들이 쓰여진 순서에 따른 **문맥 정보**를 이용할 수 없다.\n",
    "이를 해결하기 위해 문서들의 통계적인 값으로 표현하는 것이 아닌, 있는 그대로 **단어의 시퀀스**로 표현해서 처리한다.\n",
    "\n",
    "N-gram이란?\n",
    "N-gram에서 하나의 토큰은 **두 개 이상의 단어**로 구성 된다.\n",
    "uni-gram : 한 개의 단어\n",
    "bi-gram  : 두 개의 단어\n",
    "tri-gram : 세 개의 단어\n",
    "\n",
    "여기에 변수를 계속해서 추가하면, 문제를 가져오기 때문에 따라서 많아야 tri-gram까지 사용한다.\n",
    "N-gram을 도입하더라도 더 긴 단어 시퀀스로 이루어진 문맥은 여전히 파악할 수 없다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "X_train = newsgroup_train.data\n",
    "y_train = newsgroup_train.target\n",
    "\n",
    "X_test = newsgroup_test.data\n",
    "y_test = newsgroup_test.target"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 11483)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "## uni-gram\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "tfidf = TfidfVectorizer(token_pattern= \"[a-zA-Z']{3,}\", # 토큰화를 위한 정규식\n",
    "                        decode_error ='ignore',\n",
    "                        lowercase=True,\n",
    "                        stop_words = stopwords.words('english'),\n",
    "                        max_df=0.5,\n",
    "                        min_df=2)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(X_train_tfidf.shape) #(문서의 개수, 특성의 개수)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score : 0.976\n",
      "Test set score : 0.765\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "ridge_clf = RidgeClassifier()\n",
    "ridge_clf.fit(X_train_tfidf, y_train)\n",
    "print('Train set score : {:.3f}'. format(ridge_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score : {:.3f}'. format(ridge_clf.score(X_test_tfidf, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 26550)\n"
     ]
    }
   ],
   "source": [
    "## bi-gram\n",
    "tfidf = TfidfVectorizer(token_pattern= \"[a-zA-Z']{3,}\", # 토큰화를 위한 정규식\n",
    "                        decode_error ='ignore',\n",
    "                        lowercase=True,\n",
    "                        stop_words = stopwords.words('english'),\n",
    "                        ngram_range= (1, 2), #bi-gram 설정\n",
    "                        max_df=0.5,\n",
    "                        min_df=2)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(X_train_tfidf.shape) #(문서의 개수, 특성의 개수) -> 2배 정도 늘은 것을 확인할 수 있음"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi-gram samples :  [\"'cause can't\", \"'em better\", \"'expected errors'\", \"'karla' next\", \"'nodis' password\", \"'official doctrine\", \"'ok see\", \"'sci astro'\", \"'what's moonbase\", 'aas american']\n",
      "Train set score : 0.976\n",
      "Test set score : 0.773\n"
     ]
    }
   ],
   "source": [
    "bigram_features = [f for f in tfidf.get_feature_names_out() if len(f.split()) > 1]\n",
    "print('bi-gram samples : ', bigram_features[:10])\n",
    "\n",
    "ridge_clf.fit(X_train_tfidf, y_train)\n",
    "print('Train set score : {:.3f}'. format(ridge_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score : {:.3f}'. format(ridge_clf.score(X_test_tfidf, y_test))) # 성능 향상"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 32943)\n"
     ]
    }
   ],
   "source": [
    "## tri-gram\n",
    "tfidf = TfidfVectorizer(token_pattern= \"[a-zA-Z']{3,}\", # 토큰화를 위한 정규식\n",
    "                        decode_error ='ignore',\n",
    "                        lowercase=True,\n",
    "                        stop_words = stopwords.words('english'),\n",
    "                        ngram_range= (1, 3), #tri-gram 설정\n",
    "                        max_df=0.5,\n",
    "                        min_df=2)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(X_train_tfidf.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tri-gram samples :  [\"'cause can't\", \"'em better\", \"'em better shots\", \"'expected errors'\", \"'expected errors' basically\", \"'karla' next\", \"'karla' next one\", \"'nodis' password\", \"'nodis' password also\", \"'official doctrine\"]\n",
      "Train set score : 0.976\n",
      "Test set score : 0.775\n"
     ]
    }
   ],
   "source": [
    "trigram_features = [f for f in tfidf.get_feature_names_out() if len(f.split()) > 1]\n",
    "print('tri-gram samples : ', trigram_features[:10])\n",
    "\n",
    "ridge_clf.fit(X_train_tfidf, y_train)\n",
    "print('Train set score : {:.3f}'. format(ridge_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score : {:.3f}'. format(ridge_clf.score(X_test_tfidf, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 한국어 문서 분류"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  review  rating        date  \\\n0                                 돈 들인건 티가 나지만 보는 내내 하품만       1  2018.10.29   \n1           몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.      10  2018.10.26   \n2      이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...       8  2018.10.24   \n3                                    이 정도면 볼만하다고 할 수 있음!       8  2018.10.22   \n4                                                   재미있다      10  2018.10.20   \n...                                                  ...     ...         ...   \n14720  어른들을 위한 동화    정말 오랜만에  좋은 애니를 보았습니다     가족의 소중...      10  2018.01.12   \n14721                                   디즈니는 못해도 본전은 한다.       7  2018.01.12   \n14722                            가족을 위한 영화... 괜찮은 영화.~~~       8  2018.01.12   \n14723      간만에 제대로 잘짜여진 각본의 영화를 봤네 여운이 아직도 남아~어른을 위한 애니~      10  2018.01.12   \n14724                   한국개봉을 눈빠지게 기다린 보람이있다 깨우치는게 많은 영화      10  2018.01.12   \n\n        title  \n0      인피니티 워  \n1      인피니티 워  \n2      인피니티 워  \n3      인피니티 워  \n4      인피니티 워  \n...       ...  \n14720      코코  \n14721      코코  \n14722      코코  \n14723      코코  \n14724      코코  \n\n[14725 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>rating</th>\n      <th>date</th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>돈 들인건 티가 나지만 보는 내내 하품만</td>\n      <td>1</td>\n      <td>2018.10.29</td>\n      <td>인피니티 워</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.</td>\n      <td>10</td>\n      <td>2018.10.26</td>\n      <td>인피니티 워</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...</td>\n      <td>8</td>\n      <td>2018.10.24</td>\n      <td>인피니티 워</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>이 정도면 볼만하다고 할 수 있음!</td>\n      <td>8</td>\n      <td>2018.10.22</td>\n      <td>인피니티 워</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>재미있다</td>\n      <td>10</td>\n      <td>2018.10.20</td>\n      <td>인피니티 워</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14720</th>\n      <td>어른들을 위한 동화    정말 오랜만에  좋은 애니를 보았습니다     가족의 소중...</td>\n      <td>10</td>\n      <td>2018.01.12</td>\n      <td>코코</td>\n    </tr>\n    <tr>\n      <th>14721</th>\n      <td>디즈니는 못해도 본전은 한다.</td>\n      <td>7</td>\n      <td>2018.01.12</td>\n      <td>코코</td>\n    </tr>\n    <tr>\n      <th>14722</th>\n      <td>가족을 위한 영화... 괜찮은 영화.~~~</td>\n      <td>8</td>\n      <td>2018.01.12</td>\n      <td>코코</td>\n    </tr>\n    <tr>\n      <th>14723</th>\n      <td>간만에 제대로 잘짜여진 각본의 영화를 봤네 여운이 아직도 남아~어른을 위한 애니~</td>\n      <td>10</td>\n      <td>2018.01.12</td>\n      <td>코코</td>\n    </tr>\n    <tr>\n      <th>14724</th>\n      <td>한국개봉을 눈빠지게 기다린 보람이있다 깨우치는게 많은 영화</td>\n      <td>10</td>\n      <td>2018.01.12</td>\n      <td>코코</td>\n    </tr>\n  </tbody>\n</table>\n<p>14725 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/ahyeo/OneDrive/문서/바탕 화면/Project2024/TextMining/Chapter02/daum_movie_review.csv\")\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "title\n신과함께      4947\n택시운전사     2322\n인피니티 워    2042\n범죄도시      1939\n곤지암       1547\n라라랜드      1150\n코코         778\nName: count, dtype: int64"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.title.value_counts() # 데이터셋 불균형 존재 -> 언더샘플링 혹은 오버샘플링 가능"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train set size :  11043\n",
      "#test set size :  3682\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(df.review, df.title, random_state=0, test_size=0.25)\n",
    "\n",
    "print(\"#train set size : \", len(X_train))\n",
    "print(\"#test set size : \", len(X_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['몰입', '할수밖에', '없다', '.', '어렵게', '생각', '할', '필요없다', '.', '내', '가', '전투', '에', '참여', '한', '듯', '손', '에', '땀', '이남', '.']\n",
      "['몰입', '생각', '내', '전투', '참여', '듯', '손', '땀', '이남']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "print(okt.morphs(X_train[1]))\n",
    "print(okt.nouns(X_train[1])) # 일반적으로 문서를 대상으로 분석하는 경우, 명사만으로도 좋은 결과가 나오는 경우가 많다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer = okt.nouns, max_features = 2000, min_df = 5, max_df = 0.5)\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set score : 0.756\n",
      "#Test set score : 0.694\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter = 1000) # 1000번 학습 진행\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "print(\"#Train set score : {:.3f}\".format(clf.score(X_train_tfidf, y_train)))\n",
    "print(\"#Test set score : {:.3f}\".format(clf.score(X_test_tfidf, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제 영화 제목, 예측한 제목, 리뷰\n",
      "('범죄도시', '신과함께', '오랜만에 잼나는 영화 봤습니다.  다음에 더 재미있는 영화 기대하겠습니다.')\n",
      "('범죄도시', '범죄도시', '조연들이 눈에 박힌다. 간만에 집중 ㅎ')\n",
      "('코코', '코코', '대감동을 선사. 인사이드 아웃을 잇는 픽사의 감동스토리. 신과함께의 멕시코판이라고나할까요??')\n",
      "('신과함께', '신과함께', '돈이 안아까웠던 영화ᆞᆞ  정말 좋았다')\n",
      "('신과함께', '신과함께', '역시 김용화감독이 영화는 잘 만들어요. 이제 VFX 제작 부문도 헐리우드 수준 이상입니다.')\n",
      "('택시운전사', '택시운전사', '민주화를 위해 힘써주신 분들께 감사하는 마음으로 살아야겠다.')\n",
      "('신과함께', '신과함께', '잠만 자다 왔음')\n",
      "('신과함께', '신과함께', '오랜만에 잼있고 좋은 영화를 봤다')\n",
      "('범죄도시', '신과함께', '잼남')\n",
      "('범죄도시', '인피니티 워', '대박~~')\n"
     ]
    }
   ],
   "source": [
    "print(\"실제 영화 제목, 예측한 제목, 리뷰\")\n",
    "for content in zip(y_test[:10], clf.predict(X_test_tfidf[:10]), X_test[:10]):\n",
    "    print(content) # 대박은 좀.."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set score : 0.777\n",
      "#Test set score : 0.695\n"
     ]
    }
   ],
   "source": [
    "## 성능 개선\n",
    "\n",
    "# 명사뿐만 아니라 모든 형태소 사용\n",
    "tfidf = TfidfVectorizer(tokenizer=okt.morphs, max_features=2000, min_df = 5, max_df = 0.5)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter = 1000)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"#Train set score : {:.3f}\".format(clf.score(X_train_tfidf, y_train)))\n",
    "print(\"#Test set score : {:.3f}\".format(clf.score(X_test_tfidf, y_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%ㅅ\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set score : 0.784\n",
      "#Test set score : 0.712\n"
     ]
    }
   ],
   "source": [
    "def twit_tokenizer(text) : # 명사, 동사, 형용사 사용\n",
    "    target_pos = ['Noun', 'Verb', 'Adjective']\n",
    "    result = []\n",
    "    for word, tag in okt.pos(text, norm = True, stem = True): # 정규화 및 사전어휘로 변환 사용\n",
    "        if tag in target_pos:\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=twit_tokenizer, max_features=2000, min_df = 5, max_df = 0.5)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter = 1000)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"#Train set score : {:.3f}\".format(clf.score(X_train_tfidf, y_train)))\n",
    "print(\"#Test set score : {:.3f}\".format(clf.score(X_test_tfidf, y_test))) ## 성능 향상 !"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 결과\n",
    "\n",
    "품사를 선별하는 것이 성능향상에 도움이 된다는 것을 알 수 있다.\n",
    "같은 단어가 서로 다른 품사로 사용된 경우, 이를 구분하지 못한다.\n",
    "-> 단어에 품사명을 붙여 하나의 단어로 만든다"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['몰입/Noun', '하다/Verb', '없다/Adjective', './Punctuation', '어렵다/Adjective', '생각/Noun', '하다/Verb', '필요없다/Adjective', './Punctuation', '내/Noun', '가/Josa', '전투/Noun', '에/Josa', '참여/Noun', '한/Determiner', '듯/Noun', '손/Noun', '에/Josa', '땀/Noun', '이남/Noun', './Punctuation']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set score : 0.789\n",
      "#Test set score : 0.718\n"
     ]
    }
   ],
   "source": [
    "def twit_tokenizer2(text):\n",
    "    result = []\n",
    "    for word, tag in okt.pos(text, norm = True, stem = True):\n",
    "        result.append('/'.join([word, tag]))\n",
    "    return result\n",
    "\n",
    "print(twit_tokenizer2(X_train[1]))\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=twit_tokenizer2, max_features=2000, min_df = 5, max_df = 0.5)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter = 1000)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"#Train set score : {:.3f}\".format(clf.score(X_train_tfidf, y_train)))\n",
    "print(\"#Test set score : {:.3f}\".format(clf.score(X_test_tfidf, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['몰입/Noun', '하다/Verb', '없다/Adjective', '어렵다/Adjective', '생각/Noun', '하다/Verb', '필요없다/Adjective', '내/Noun', '전투/Noun', '참여/Noun', '듯/Noun', '손/Noun', '땀/Noun', '이남/Noun']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set score : 0.784\n",
      "#Test set score : 0.713\n"
     ]
    }
   ],
   "source": [
    "def twit_tokenizer2(text):\n",
    "    result = []\n",
    "    target_pos = ['Noun', 'Verb', 'Adjective']\n",
    "    for word, tag in okt.pos(text, norm=True, stem=True):\n",
    "        if tag in target_pos:\n",
    "            result.append('/'.join([word, tag]))\n",
    "    return result\n",
    "\n",
    "print(twit_tokenizer2(X_train[1]))\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=twit_tokenizer2, max_features=2000, min_df=5, max_df=0.5)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"#Train set score : {:.3f}\".format(clf.score(X_train_tfidf, y_train)))\n",
    "print(\"#Test set score : {:.3f}\".format(clf.score(X_test_tfidf, y_test))) # 엥 성능 안높아짐.."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Max alpha 2.300 at max validation score 0.717\n"
     ]
    }
   ],
   "source": [
    "## 그리드 서치\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_ridge, X_val_ridge, y_train_ridge, y_val_ridge = train_test_split(X_train_tfidf, y_train, test_size = 0.2, random_state=42)\n",
    "\n",
    "max_score = 0\n",
    "max_alpha = 0\n",
    "\n",
    "for alpha in np.arange(0.1, 10, 0.1):\n",
    "    ridge_clf = RidgeClassifier(alpha = alpha)\n",
    "    ridge_clf.fit(X_train_ridge, y_train_ridge)\n",
    "    score = ridge_clf.score(X_val_ridge, y_val_ridge)\n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "        max_alpha = alpha\n",
    "\n",
    "print(\"#Max alpha {:.3f} at max validation score {:.3f}\".format(max_alpha, max_score))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Ridge Train set score : 0.786\n",
      "#Ridge Test set score : 0.717\n",
      "#Lasso Train set score : 0.700\n",
      "#Lasso Test set score : 0.695\n"
     ]
    }
   ],
   "source": [
    "# 릿지 회귀분석\n",
    "ridge_clf = RidgeClassifier(alpha = 1.6)\n",
    "ridge_clf.fit(X_train_ridge, y_train_ridge)\n",
    "print(\"#Ridge Train set score : {:.3f}\".format(ridge_clf.score(X_train_tfidf, y_train)))\n",
    "print(\"#Ridge Test set score : {:.3f}\".format(ridge_clf.score(X_test_tfidf, y_test)))\n",
    "\n",
    "# 라쏘 회귀분석\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "lasso_clf = LogisticRegression(penalty='l1', solver = 'liblinear', C= 0.5)\n",
    "lasso_clf.fit(X_train_tfidf, y_train)\n",
    "print(\"#Lasso Train set score : {:.3f}\".format(lasso_clf.score(X_train_tfidf, y_train)))\n",
    "print(\"#Lasso Test set score : {:.3f}\".format(lasso_clf.score(X_test_tfidf, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set score : 0.773\n",
      "#Test set score : 0.711\n"
     ]
    }
   ],
   "source": [
    "# 나이브베이즈\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "NB_clf = MultinomialNB(alpha = 0.1)\n",
    "NB_clf.fit(X_train_tfidf, y_train)\n",
    "print(\"#Train set score : {:.3f}\".format(NB_clf.score(X_train_tfidf, y_train)))\n",
    "print(\"#Test set score : {:.3f}\".format(NB_clf.score(X_test_tfidf, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
