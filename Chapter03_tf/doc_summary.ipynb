{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 주의사항\n",
    "\n",
    "text_mining_ver02에서 kobert와 transformer 의존성 충돌로 인해\n",
    "문서 요약은 text_mining_ver01에서 수행하였음"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 문서 요약\n",
    "\n",
    "문서요약은 주어진 문서가 담고 있는 중요한 내용을 요약하여 짧은 텍스트를 생성하는 작업을 말한다.\n",
    "\n",
    "### 문서 요약의 이해\n",
    "문서 요약은 추출 요약(Extractive Summarization)과  생성 요약(Abstractive Summarization)으로 구분된다.\n",
    "추출 요약 : 원본 문서에 있는 주요 문장 혹은 단어를 추출하여 요약문을 작성한다.\n",
    "문장의 중요도를 평가해 순위를 매긴 후, 상위의 문장을 선택한다.\n",
    "전체 입력 텍스트와 코사인 유사도가 높은 문장을 정렬한 후에 상위 문장 2개로 요약문을 만든다.\n",
    "\n",
    "생성 요약 : 원본 문서에 있지 않은 문장이나 단어를 생성해 요약문을 만들기 때문에 추출 요약에 비해 상대적으로 난이도가 높다.\n",
    "원본 텍스트에 대한 이해를 전제로 하여 요약문을 새로 생성하기 때문에 Seq2Seq 모형을 사용한다"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 문서 요약 성능 지표 : ROUGE\n",
    "\n",
    "ROUGE : 모형에 의해 요약된 요약문을 전문가가 요약한 요약문, 즉 정답과 비교하여 성능을 측정한다.\n",
    "단어의 출현 순서와 일치하는 정도를 정밀도과 재현율, F1Score로 측정한다.\n",
    "즉, 전문가가 사용한 단어와 만들어낸 단어가 얼마나 일치하는지 보는 것이다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 데이터셋\n",
    "한국의 경우, AIHub에서 \"요약문 및 레포트 생성 데이터\"라는 추출요약문과 생성요약문을 함께 제공하고 있다.\n",
    "문서 요약을 지원하는 트랜스포머 변형 모형으로 여러가지가 있는데, MBart-50은 한국어 요약을 제공한다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 파이프라인을 이용한 문서 요약"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-small and revision d769bba (https://huggingface.co/google-t5/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Your max_length is set to 200, but your input_length is only 146. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=73)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요약문:\n",
      " [{'summary_text': 'text data mining is the process of deriving high-quality information from text . it involves the discovery by computer of new, previously unknown information . a KDD (Knowledge Discovery in Databases) process is similar to text analytics .'}]\n",
      "원문 길이: 771 요약문 길이: 239\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 문서요약을 위한 파이프라인 생성\n",
    "summarizer = pipeline(\"summarization\", framework=\"tf\")\n",
    "# 요약 대상 원문 - 텍스트마이닝의 정의(Wikipedia)\n",
    "text = '''Text mining, also referred to as text data mining (abbr.: TDM), similar to text analytics,\n",
    "        is the process of deriving high-quality information from text. It involves\n",
    "        \"the discovery by computer of new, previously unknown information,\n",
    "        by automatically extracting information from different written resources.\"\n",
    "        Written resources may include websites, books, emails, reviews, and articles.\n",
    "        High-quality information is typically obtained by devising patterns and trends\n",
    "        by means such as statistical pattern learning. According to Hotho et al. (2005)\n",
    "        we can distinguish between three different perspectives of text mining:\n",
    "        information extraction, data mining, and a KDD (Knowledge Discovery in Databases) process.'''\n",
    "summary_text = summarizer(text) #파이프라인으로 문서요약 수행\n",
    "print(\"요약문:\\n\", summary_text)\n",
    "print(\"원문 길이:\", len(text), \"요약문 길이:\", len(summary_text[0][\"summary_text\"]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "사용할 모델을 정하지 않아 Summarization에 default로 \"DistilBART\"를 사용하였다.\n",
    "DistilBART는 BART를 경량화한 버전이다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### T5 모형과 자동 클래스를 이용한 문서 요약\n",
    "\n",
    "BERT와 GPT가 각각 트랜스포머의 인코더와 디코더만을 사용해서 모형을 만들어다면, T5는 인코더와 디코더를 모두 사용하여 다양한 자연어 처리 작업에 사용될 수 있는\n",
    "통합 text-to-text 구조를 제안하였다.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer type: <class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>\n"
     ]
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bade13ef12e44c6f835c653f2a6e0d8e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2cd50d8e25f4465f956ead641846bd98"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model type: <class 'transformers.models.t5.modeling_tf_t5.TFT5ForConditionalGeneration'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\", model_max_length=512)\n",
    "print(\"tokenizer type:\", type(tokenizer))\n",
    "\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "print(\"model type:\", type(model))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "preprocess_text = text.strip().replace(\"\\n\", \"\")\n",
    "input_text = \"summarize : \" + preprocess_text\n",
    "\n",
    "tokenized_text = tokenizer.encode(input_text, return_tensors = \"tf\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized text : \n",
      " text data mining is the process of deriving high-quality information from text. it involves the discovery by computer of new, previously unknown information. a KDD (Knowledge Discovery in Databases) process is similar to text analytics.\n",
      "Origin text length :  771 Summarized text length :  236\n"
     ]
    }
   ],
   "source": [
    "summary_ids = model.generate(tokenized_text,\n",
    "                             num_beams = 4, #가장 나올 확률이 높은 단어들 선정 -> 하지만, 더 좋은 확률을 가진 단어가 다음에 숨어있을 수도 있기 때문에 4개의 경우의 수를 저장한다.\n",
    "                             no_repeat_ngram_size = 3, #한 번 \"the dog is\"가 만들어졌으면, 요약문에 다시 나올 수 없다.\n",
    "                             min_length = 30,\n",
    "                             max_length = 100,\n",
    "                             early_stopping = True)\n",
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True) #문장의 종류를 나타내는 토큰이 선택되면 생성을 종료할지 여부\n",
    "\n",
    "print(\"Summarized text : \\n\", output)\n",
    "print(\"Origin text length : \", len(text), \"Summarized text length : \", len(output))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text:\n",
      " Das ist gut.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"translate English to German: That is good\"\n",
    "\n",
    "tokenized_text = tokenizer.encode(input_text, return_tensors=\"tf\")\n",
    "\n",
    "result = model.generate(tokenized_text,\n",
    "                        num_beams=4,\n",
    "                        no_repeat_ngram_size=3,\n",
    "                        max_length=100,\n",
    "                        early_stopping=True)\n",
    "\n",
    "output = tokenizer.decode(result[0], skip_special_tokens=True)\n",
    "print(\"Translated text:\\n\", output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### T5 모형과 트레이너를 이용한 미세조정학습"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import T5TokenizerFast, TFT5ForConditionalGeneration\n",
    "\n",
    "model = TFT5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5-small\", model_max_length = 1024)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized text : \n",
      " Das ist gut so, wie es der Fall ist, d. h. daß dies nicht der Fall sein wird.\n",
      "Original text length :  441 Summarized text length :  77\n"
     ]
    }
   ],
   "source": [
    "text = '''The Inflation Reduction Act lowers prescription drug costs, health care costs,\n",
    "and energy costs. It's the most aggressive action on tackling the climate crisis in American history,\n",
    "which will lift up American workers and create good-paying, union jobs across the country.\n",
    "It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share.\n",
    "And no one making under $400,000 per year will pay a penny more in taxes.'''\n",
    "\n",
    "preprocess_text = text.strip().replace(\"\\n\", \"\")\n",
    "input_text = \"Summarize : \"+preprocess_text\n",
    "\n",
    "preprocessed_text = tokenizer.encode(input_text, return_tensors=\"tf\")\n",
    "summary_ids = model.generate(tokenized_text,\n",
    "                             num_beams = 4,\n",
    "                             no_repeat_ngram_size = 3,\n",
    "                             min_length = 30,\n",
    "                             max_length = 100,\n",
    "                             early_stopping = True)\n",
    "\n",
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens = True)\n",
    "\n",
    "print(\"Summarized text : \\n\", output)\n",
    "print(\"Original text length : \", len(text), \"Summarized text length : \", len(output))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/6.87k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b10d79bd8a8440e79537891c71d22ad7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/91.8M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5be9fa8af96d437a9f90a159020dc691"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/15.8M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77621c26f4ea46e796368ec212ea8286"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/6.12M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9440c66af056457ab2f1205228746f88"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/18949 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ecc473e4ec444810b094965843d6750c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/3269 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56ba72b4a51b4a75a6873f2bbdeccb20"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating ca_test split:   0%|          | 0/1237 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d12e6d89ca44ea891c2e826d9462cfc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Billsum의 데이터 예 - 첫 항목\n",
      "\tText: The people of the State of California do enact as \n",
      "\tSummary: Existing law establishes the California Homebuyer’\n",
      "\tTitle: An act to amend Section 51504 of the Health and Sa\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "billsum = load_dataset(\"billsum\", split = \"ca_test\")\n",
    "billsum = billsum.train_test_split(test_size = 0.2)\n",
    "example = billsum[\"train\"][0]\n",
    "print(\"Billsum의 데이터 예 - 첫 항목\")\n",
    "print(\"\\tText:\", example['text'][:50])\n",
    "print(\"\\tSummary:\", example['summary'][:50])\n",
    "print(\"\\tTitle:\", example['title'][:50])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/989 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d905eb04b8b24d5f9381e8c5e6e50ae3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/248 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac1d82e041fa44c8bd1e7d90dec22a4b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 989\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 248\n    })\n})"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(data):\n",
    "    inputs = [\"Summarize : \" + doc for doc in data[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "    labels = tokenizer(data[\"summary\"], max_length = 128, truncation = True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_billsum = billsum.map(preprocess_text, batched = True, remove_columns=billsum[\"train\"].column_names)\n",
    "tokenized_billsum"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#디코더에 필요한 라벨 입력을 자동으로 생성하는 역할\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer, model = model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d8a9febf8be4a5ea676a6e8a6331445"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#rouge를 통한 성능 평가\n",
    "import numpy as np\n",
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # 생성한 요약 토큰을 텍스트로 디코드\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # 라벨에서 디코드할 수 없는 -100을 교체\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # 라벨을 텍스트로 디코드 -> 전문가가 생성한 정답\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # 디코드된 요약문과 라벨로 ROUGE 스코어 계산\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nSeq2SeqTrainer requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFSeq2SeqTrainer\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 17\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Seq2SeqTrainingArguments, Seq2SeqTrainer\n\u001B[0;32m      5\u001B[0m training_args \u001B[38;5;241m=\u001B[39m Seq2SeqTrainingArguments(\n\u001B[0;32m      6\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./summary\u001B[39m\u001B[38;5;124m\"\u001B[39m,         \u001B[38;5;66;03m# 모형 예측과 체크포인트 저장 폴더\u001B[39;00m\n\u001B[0;32m      7\u001B[0m     evaluation_strategy\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m\"\u001B[39m,    \u001B[38;5;66;03m# 평가 단위, 여기서는 epoch를 선택\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     14\u001B[0m     predict_with_generate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,     \u001B[38;5;66;03m# 평가지표(ROUGE) 계산을 위해 generate할 지의 여부 -> compute metrice를 사용할 것인지\u001B[39;00m\n\u001B[0;32m     15\u001B[0m )\n\u001B[1;32m---> 17\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[43mSeq2SeqTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenized_billsum\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenized_billsum\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtest\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_collator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_collator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompute_metrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompute_metrics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\transformers\\utils\\dummy_pt_objects.py:9621\u001B[0m, in \u001B[0;36mSeq2SeqTrainer.__init__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   9620\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m-> 9621\u001B[0m     \u001B[43mrequires_backends\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtorch\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\transformers\\utils\\import_utils.py:1309\u001B[0m, in \u001B[0;36mrequires_backends\u001B[1;34m(obj, backends)\u001B[0m\n\u001B[0;32m   1307\u001B[0m \u001B[38;5;66;03m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001B[39;00m\n\u001B[0;32m   1308\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m backends \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m backends \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_available() \u001B[38;5;129;01mand\u001B[39;00m is_tf_available():\n\u001B[1;32m-> 1309\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001B[38;5;241m.\u001B[39mformat(name))\n\u001B[0;32m   1311\u001B[0m \u001B[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001B[39;00m\n\u001B[0;32m   1312\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m backends \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m backends \u001B[38;5;129;01mand\u001B[39;00m is_torch_available() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_tf_available():\n",
      "\u001B[1;31mImportError\u001B[0m: \nSeq2SeqTrainer requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFSeq2SeqTrainer\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ],
   "source": [
    "##미세조정 -> 이건 파이토치로만 가능\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./summary\",         # 모형 예측과 체크포인트 저장 폴더\n",
    "    evaluation_strategy=\"epoch\",    # 평가 단위, 여기서는 epoch를 선택\n",
    "    learning_rate=2e-5,             # 학습률\n",
    "    per_device_train_batch_size=16, # 학습에 사용할 배치 크기\n",
    "    per_device_eval_batch_size=16,  # 평가에 사용할 배치 크기\n",
    "    weight_decay=0.01,              # 가중치 감쇠 값\n",
    "    save_total_limit=3,             # 저장할 체크포인트의 최대값\n",
    "    num_train_epochs=4,             # 에포크 수\n",
    "    predict_with_generate=True,     # 평가지표(ROUGE) 계산을 위해 generate할 지의 여부 -> compute metrice를 사용할 것인지\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_billsum[\"train\"],\n",
    "    eval_dataset=tokenized_billsum[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 한글 문서 요약"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "text = \"\"\"디아블로는 액션 롤플레잉 핵 앤드 슬래시 비디오 게임이다.\n",
    "플레이어는 주변 환경을 마우스로 사용해 영웅을 움직이게 한다.\n",
    "주문을 외는 등의 다른 활동은 키보드 입력으로 이루어진다.\n",
    "플레이어는 이 게임에서 장비를 획득하고, 주문을 배우고, 적을 쓰러뜨리며, NPC와 대화를 나눌 수 있다.\n",
    "지하 미궁은 주어진 형식이 있고 부분적으로 반복되는 형태가 존재하나 전체적으로 보면 무작위로 생성된다.\n",
    "예를 들어 지하 묘지의 경우에는 긴 복도와 닫힌 문들이 존재하고, 동굴은 좀 더 선형 형태를 띠고 있다.\n",
    "플레이어에게는 몇몇 단계에서 무작위의 퀘스트를 받는다.\n",
    "이 퀘스트는 선택적인 사항이나 플레이어의 영웅들을 성장시키거나 줄거리를 이해하는데 도움을 준다.\n",
    "그러나 맨 뒤에 두 퀘스트는 게임을 끝내기 위해 완료시켜야 한다.\"\"\"\n",
    "\n",
    "preprocess_text = text.strip().replace(\"\\n\",\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디아블로는 액션 롤플레잉 핵 앤드 슬래시 비디오 게임이다.플레이어는 주변 환경을 마우스로 사용해 영웅을 움직이게 한다.주문을 외는 등의 다른 활동은 키보드 입력으로 이루어진다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast, TFBartForConditionalGeneration\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-summarization')\n",
    "model = TFBartForConditionalGeneration.from_pretrained('gogamza/kobart-summarization')\n",
    "\n",
    "# 전처리된 텍스트 토큰화\n",
    "tokenized_text = tokenizer.encode(preprocess_text, return_tensors=\"tf\")\n",
    "\n",
    "# 요약 생성\n",
    "summary_ids = model.generate(tokenized_text,\n",
    "                             num_beams=4,  # beam의 길이\n",
    "                             no_repeat_ngram_size=3,  # 동어 반복을 피하기 위해 사용\n",
    "                             min_length=10,  # 요약문의 최소 토큰 수\n",
    "                             max_length=150,  # 요약문의 최대 토큰 수\n",
    "                             early_stopping=True)  # EOS 토큰을 만나면 종료\n",
    "\n",
    "# 요약문 디코딩\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(summary)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading a PyTorch model in TensorFlow, requires both PyTorch and TensorFlow to be installed. Please see https://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoTokenizer, TFAutoModelForSeq2SeqLM\n\u001B[0;32m      3\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsebuetnlp/mT5_multilingual_XLSum\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mTFAutoModelForSeq2SeqLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsebuetnlp/mT5_multilingual_XLSum\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_pt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# 전처리된 텍스트를 토큰화하고 TensorFlow 텐서로 변환\u001B[39;00m\n\u001B[0;32m      7\u001B[0m tokenized_text \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mencode(preprocess_text, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:561\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    559\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    560\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[1;32m--> 561\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    562\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    563\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    564\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    565\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    566\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    567\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\transformers\\modeling_tf_utils.py:2880\u001B[0m, in \u001B[0;36mTFPreTrainedModel.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m   2877\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodeling_tf_pytorch_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_pytorch_checkpoint_in_tf2_model\n\u001B[0;32m   2879\u001B[0m     \u001B[38;5;66;03m# Load from a PyTorch checkpoint\u001B[39;00m\n\u001B[1;32m-> 2880\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mload_pytorch_checkpoint_in_tf2_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2881\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2882\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresolved_archive_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2883\u001B[0m \u001B[43m        \u001B[49m\u001B[43mallow_missing_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   2884\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_loading_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_loading_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2885\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_prefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mload_weight_prefix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2886\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtf_to_pt_weight_rename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf_to_pt_weight_rename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2887\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2889\u001B[0m \u001B[38;5;66;03m# we might need to extend the variable scope for composite models\u001B[39;00m\n\u001B[0;32m   2890\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m load_weight_prefix \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\transformers\\modeling_tf_pytorch_utils.py:168\u001B[0m, in \u001B[0;36mload_pytorch_checkpoint_in_tf2_model\u001B[1;34m(tf_model, pytorch_checkpoint_path, tf_inputs, allow_missing_keys, output_loading_info, _prefix, tf_to_pt_weight_rename)\u001B[0m\n\u001B[0;32m    166\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    167\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n\u001B[1;32m--> 168\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n\u001B[0;32m    169\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msafetensors\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_file \u001B[38;5;28;01mas\u001B[39;00m safe_load_file  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n\u001B[0;32m    171\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpytorch_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_torch_greater_or_equal_than_1_13  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/mT5_multilingual_XLSum\")\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"csebuetnlp/mT5_multilingual_XLSum\", from_pt=True)\n",
    "\n",
    "# 전처리된 텍스트를 토큰화하고 TensorFlow 텐서로 변환\n",
    "tokenized_text = tokenizer.encode(preprocess_text, return_tensors=\"tf\")\n",
    "\n",
    "# 요약 생성\n",
    "summary_ids = model.generate(tokenized_text,\n",
    "                             num_beams=4,  # beam의 길이\n",
    "                             no_repeat_ngram_size=2,  # 동어 반복을 피하기 위해 사용\n",
    "                             min_length=10,  # 요약문의 최소 토큰 수\n",
    "                             max_length=150,  # 요약문의 최대 토큰 수\n",
    "                             early_stopping=True)  # EOS 토큰을 만나면 종료\n",
    "\n",
    "# 요약문 디코딩\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(summary)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
