{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## CNN\n",
    "\n",
    "원래 이미지 인식에 탁월한 효과를 보이는 딥러닝 신경망이지만, 문서 분류에 뛰어난 효과를 보이는 것으로 입증되어 텍스트 마이닝 분야에서도\n",
    "활발하게 쓰이는 모형이 되었다.\n",
    "BERT의 결과와 CNN을 결합한 모형이 제안되는 등 여전히 많이 사용된다.\n",
    "\n",
    "CNN : 2차원 행렬로부터 주변 정보를 요약해 이미지를 분류할 수 있는 특성들을 추출하는 딥러닝 기법이다. (컬러 이미지 : 3차원 행렬)\n",
    "\n",
    "**CNN 구조**\n",
    "1 . 컨볼루션 단계\n",
    "원본 이미지에 필터를 적용해 주변 정보를 요약하는 단계\n",
    "필터는 원본 이미지보다 작은 2차원 행렬로 학습의 대상이 되는 파라미터 (파라미터에 따라 요약정보는 달라짐. weight의 느낌)\n",
    "이 단계에서 중요한 요소는 필터 혹은 채너의 수이다.\n",
    "필터를 2개를 사용한다면, 두 개의 요약된 2차원 행렬을 만들어낼 수 있다. (하나의 이미지 -> 2가지의 다른 정보 추출)\n",
    "2 . 풀링 단계\n",
    "요약된 2차원 행렬을 축소하는 단계\n",
    "max pooling의 경우 각 필터 범위 안에 있는 가장 높은 값을 추출해내 새로운 2차원 행렬을 만든다. (이미지 너비 축소)\n",
    "\n",
    "이 특성을 통해 이미지의 정보를 축략한 특성들을 만들어내고, 모형의 마지막 단계에서 이 특성을 가지고 이미지를 판별한다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CNN을 이용한 문서 분류\n",
    "\n",
    "텍스트 마이닝에 딥러닝을 적용하는 이유는 **문맥 파악**이며, CNN을 사용하는 이유는 단어들의 연속된 나열에 대해 앞 뒤 단어들 간의 주변정보를 요약해낼 수 있다는 것이다.\n",
    "\n",
    "문서에서는 단어들의 나열을 보기 때문에 **2차원이 아닌 1차원 방향으로만** 필터를 이동시키면서 컨볼루션을 수행한다.\n",
    "(옆/아래로 이동이 아닌 아래로만 이동)\n",
    "\n",
    "문서는 한 단어가 하나의 행으로 반환되어 단어들은 순서에 따라 **아래방향**으로 나열된다.\n",
    "여기서 단어들 간의 주변 정보를 요약하기 위해서 필터의 열 수를 단어 벡터의 수와 같게 하고(단어의 개수 -> dictionary에 있는 단어의 전체 개수/ 혹은 밀집벡터)\n",
    "단어의 나열 방향에 따라 필터를 **아래**로만 이동하며 주변 정보를 요약한다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 22\u001B[0m\n\u001B[0;32m     19\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m Tokenizer(num_words \u001B[38;5;241m=\u001B[39m max_words, oov_token \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUNK\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     20\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39mfit_on_texts(reviews) \u001B[38;5;66;03m#단어 인덱스 구축\u001B[39;00m\n\u001B[1;32m---> 22\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtexts_to_sequences\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreviews\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m#만들어진 단어 인덱스를 이용해 변환\u001B[39;00m\n\u001B[0;32m     23\u001B[0m X\u001B[38;5;241m=\u001B[39m pad_sequences(X, maxlen \u001B[38;5;241m=\u001B[39m maxlen, truncating \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpre\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     25\u001B[0m label_dict \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpos\u001B[39m\u001B[38;5;124m'\u001B[39m:\u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mneg\u001B[39m\u001B[38;5;124m'\u001B[39m:\u001B[38;5;241m1\u001B[39m}\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\keras\\src\\preprocessing\\text.py:357\u001B[0m, in \u001B[0;36mTokenizer.texts_to_sequences\u001B[1;34m(self, texts)\u001B[0m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtexts_to_sequences\u001B[39m(\u001B[38;5;28mself\u001B[39m, texts):\n\u001B[0;32m    346\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Transforms each text in texts to a sequence of integers.\u001B[39;00m\n\u001B[0;32m    347\u001B[0m \n\u001B[0;32m    348\u001B[0m \u001B[38;5;124;03m    Only top `num_words-1` most frequent words will be taken into account.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    355\u001B[0m \u001B[38;5;124;03m        A list of sequences.\u001B[39;00m\n\u001B[0;32m    356\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 357\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtexts_to_sequences_generator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\keras\\src\\preprocessing\\text.py:386\u001B[0m, in \u001B[0;36mTokenizer.texts_to_sequences_generator\u001B[1;34m(self, texts)\u001B[0m\n\u001B[0;32m    384\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    385\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39manalyzer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 386\u001B[0m         seq \u001B[38;5;241m=\u001B[39m \u001B[43mtext_to_word_sequence\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    387\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    388\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfilters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfilters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    389\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlower\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlower\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    390\u001B[0m \u001B[43m            \u001B[49m\u001B[43msplit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    391\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    392\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    393\u001B[0m         seq \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39manalyzer(text)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\keras\\src\\preprocessing\\text.py:80\u001B[0m, in \u001B[0;36mtext_to_word_sequence\u001B[1;34m(input_text, filters, lower, split)\u001B[0m\n\u001B[0;32m     77\u001B[0m translate_map \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m\u001B[38;5;241m.\u001B[39mmaketrans(translate_dict)\n\u001B[0;32m     78\u001B[0m input_text \u001B[38;5;241m=\u001B[39m input_text\u001B[38;5;241m.\u001B[39mtranslate(translate_map)\n\u001B[1;32m---> 80\u001B[0m seq \u001B[38;5;241m=\u001B[39m \u001B[43minput_text\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     81\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [i \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m seq \u001B[38;5;28;01mif\u001B[39;00m i]\n",
      "\u001B[1;31mMemoryError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "### CNN을 이용한 NLTK 영화 리뷰\n",
    "from nltk.corpus import movie_reviews\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fileids = movie_reviews.fileids()\n",
    "reviews = [movie_reviews.raw(fileids) for fileid in fileids]\n",
    "categories = [movie_reviews.categories(fileid)[0] for fileid in fileids]\n",
    "\n",
    "np.random.seed(7)\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "max_words = 1000\n",
    "maxlen = 500\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_words, oov_token = \"UNK\")\n",
    "tokenizer.fit_on_texts(reviews) #단어 인덱스 구축\n",
    "\n",
    "X = tokenizer.texts_to_sequences(reviews) #만들어진 단어 인덱스를 이용해 변환\n",
    "X= pad_sequences(X, maxlen = maxlen, truncating = 'pre')\n",
    "\n",
    "label_dict = {'pos':0, 'neg':1}\n",
    "y = np.array([label_dict[c] for c in categories])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential([\n",
    "    #word embedding layer\n",
    "    Embedding(max_words, 64, input_length = maxlen),   #500X64의 행렬로 변환 (500은 maxlen)\n",
    "    Conv1D(128, #채널의 수 128개\n",
    "           5, #필터의 크기 5x5\n",
    "           padding = 'valid', #패딩 없음\n",
    "           activation = 'relu',\n",
    "           strides = 1), #필터 이동 간격 : 1\n",
    "    MaxPooling1D(),\n",
    "    Conv1D(256, #채널의 수를 256개로 늘림\n",
    "           5,\n",
    "           padding = 'valid',\n",
    "           activation = 'relu',\n",
    "           strides = 1),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate = 1e-3)\n",
    "model.compile(optimizer = adam, loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs = 20,\n",
    "                    batch_size = 256,\n",
    "                    verbose = 0,\n",
    "                    validation_split = 0.2)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_results(history, metric):\n",
    "    plt.plot(history.history[metric], 'b', label = 'Training ' + metric)\n",
    "    plt.plot(history.history['val_'+metric], 'r--', label = 'validation '+ metric)\n",
    "    plt.title('Training. vs Validation. '+metric)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_results(history, 'loss')\n",
    "\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy : {score[1]:.3f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 결과 분석\n",
    "500개나 되는 단어들로 문서가 이루어져 있을 때, 그 500개의 단어들을 다 연결해 문맥을 파악하는 것이 더 효율적인지, 아니면 5개의 단어 정도로 끊어가며 좁은 범위에서 문맥을 파악하고 그 결과를 합치는 것이 더 효율적인지는 알 수 없다."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
