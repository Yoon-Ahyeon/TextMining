{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Attention And Transformer\n",
    "\n",
    "### Seq2seq\n",
    "입력으로 일련의 단어들이 들어오고 이를 이용해서 다시 일련의 단어들을 생성해야 하는 문제를 말한다.\n",
    "**과정**\n",
    "문장에 대한 내용을 이해 → 정해진 형태(벡터)로 이해된 내용 저장 → 번역의 첫 단어 예측(사전에 있는 단어들에 대한 확률 계산) → 높은 확률의 단어 제안\n",
    "→ 벡터와 첫 단어를 통해 다음 단어 예측 → 반복\n",
    "\n",
    "**구조**\n",
    "Encoder and Decoder\n",
    "예를 들어 번역으로 보자면, (영어 → 한국어)\n",
    "Encoder : 영어 문장을 이해하는 역할 (번역하고자 하는 문장)\n",
    "\n",
    "은닉층을 통해 앞 단어로부터 순차적으로 정보가 축적되고 마지막 <end> 입력을 받은 은닉층의 노드는 영어 문장 전체의 문맥 정보를 내포한다.\n",
    "→ **마지막 노드**에 전체적인 정보가 들어가 있다.\n",
    "\n",
    "Decoder : 이 문맥정보로부터 한국어 문장을 생성하는 역할을 한다.\n",
    "<start> : 문장의 시작 혹은 번역의 시작을 알리는 벡터\n",
    "디코더는 한 번에 실행되지 않고 각 단어를 예측하는 단계가 **순차적으로** 실행된다.\n",
    "<end> : 번역을 마친다.\n",
    "\n",
    "결론 : Encoder가 문맥정보를 축적하면, Decoder는 이 문맥정보에 자신이 생성한 단어들을 하나씩 결합하면서 다음 단어를 예측하게 되고,\n",
    "생성해야 할 단어의 수만큼 모형을 이용한 예측이 반복된다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Attention을 이용한 성능 향상\n",
    "\n",
    "#### seq2seq의 문제점\n",
    "문맥 정보가 인코더의 마지막 벡터 하나에 집중되는 현상이 나타난다.\n",
    "이 하나의 벡터에 번역할 무장의 모든 문맥 정보가 포함되고, 이를 이용해 디코더에서 반복적으로 다음 단어를 예측한다.\n",
    "만약, 번역해야할 문장이 엄청 길다면, 하나의 벡터에 많은 정보가 잘 축적되고 또 이를 디코더에서 풀어내기가 어렵다. (성능이 떨어짐)\n",
    "\n",
    "#### Attention\n",
    "하나의 단어가 단어를 번역할 때 직접 관여할 수 있도록 한다.\n",
    "Encoder의 윗부분에 **context vector**를 생성한다.\n",
    "이 벡터는 첫 단어의 예측에 가장 많은 영향을 미치는 단어에 대한 정보가 담겨있다.\n",
    "단어별로 **Attention Score**를 계산하고 softmax를 적용한 가중치를 표현한다. (가중치가 가장 높은 단어가 현재 예측해야할 단어에 가장 영향을 많이 미치는 것)\n",
    "이 단어는 학습을 통해 결정된다.\n",
    "\n",
    "결론 : 단어들의 임베딩 벡터에 대해 가중치를 반영해 가중 합계를 구하면, context vector가 되고 이 context vector가 encoder의 마지막 벡터와 deoder의 입력값과 합쳐져 첫 단어를 생성한다. **context vecotr는 당연히 예측할 단어의 순서에 따라 계속 변화한다.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Self-Attention\n",
    "\n",
    "예시. The animal didn't cross the street because it was too tired.\n",
    "VS The animal didn't cross the street because it was too wide.\n",
    "\n",
    "여기서 it이 가리키는 명사는 다르다. (animal - street)\n",
    "이와 같이 동일한 단어라도 **문맥에 따라 다른 의미를 갖게 되고** 그 의미에 영향을 미치는 단어가 문장에 존재한다.\n",
    "\n",
    "셀프 어텐션의 목적 : 문장 내에서 단어 간 영향을 표현하는 것\n",
    "어떤 단어를 벡터로 임베딩할 때, 그 단어에 영향을 미치는 다른 단어들의 정보를 **함께 인코딩**하는 것이다.\n",
    "(각 단어들에 대해 그 단어에 영향을 미치는 단어들의 정보를 선별해(가중치) 자신에게 축적한다.)\n",
    "→ 이렇게 되면, 어느 한 벡터가 전체 문맥에 대한 정보를 축적하고 있지 않다. (모두 자기 자신에 대한 벡터를 가지고 있음)\n",
    "Encoding 과정에서 문맥에 대한 정보는 각 단어에 골고루 분포하고 Decoding 과정에서는 입력 문장의 **모든 단어의 임베딩 벡터**를 어텐션 형태로 사용한다.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transformer\n",
    "\n",
    "Transformer는 기존 seq2seq 모형에서 RNN 혹은 LSTM 구조를 다 버리고 오직 Attention에만 의지한 모형을 제안하였다.\n",
    "\n",
    "#### Transformer 구조\n",
    "Encoder : 번역의 대상이 되는 문장을 입력받아 문맥 정보, 즉 셀프 어텐션 정보를 추출한다.\n",
    "Decoder : 셀프 어텐션 정보를 갖고 있는 각 단어의 임베딩 벡터를 이용해 단어를 하나씩 예측한다.\n",
    "이 때, Encoder로부터 오는 정보뿐만 아니라 자신의 셀프 어텐션 정보를 함께 사용한다.\n",
    "\n",
    "1 . Transformer 기반의 토큰화\n",
    "\n",
    "- 문자 기반 토큰화\n",
    "- BPE\n",
    "- WordPiece\n",
    "- SentencePiece\n",
    "\n",
    "출력 sequence에서도 동일하게 토큰 임베딩이 수행된다.\n",
    "예를 들어 \"Attention is all you need\"인 경우,\n",
    "첫번째 예측 : Attention → \"토큰 임베딩 : Attention\"\n",
    "두번째 예측 : is → \"토큰 임베딩 : Attention is\"\n",
    "세번째 예측 : all → \"토큰 임베딩 : Attention is all\" 이런식으로 이루어지고 그 다음 단어를 예측한다.\n",
    "\n",
    "2 . 위치 인코딩 (Positional Encoding)\n",
    "\n",
    "트랜스포머는 모든 토큰 사이에 동등한 self-attention이 동작하기 때문에 토큰의 순서와 관련이 없는 모형이다.\n",
    "따라서, 위치에 대한 정보를 토큰에 추가하는 작업을 수행한다.\n",
    "\n",
    "3 . n회 반복\n",
    "\n",
    "층이 겹쳐져 있는 경우, 앞 층의 결과는 다음 층의 입력으로 사용된다.\n",
    "하나의 층은 멀티헤드 어텐션과 피드포워드 신경망으로 구성되어 있다.\n",
    "\n",
    "**multi-head-attention** : self-attention이 병렬로 여러 개 사용되는 것\n",
    "(다양한 관점에서 self-attention을 구현하고 이를 결합해 최종 attention을 만든다.)\n",
    "이 과정을 통해 단어에 영향을 미치는 단어들의 정보가 결합된 벡터가 추출된다. (단어의 정보 크기 조절)\n",
    "→ 예를 들어, 서로 다른 여러 사람이 각자의 관점으로 문장을 해석하고, 그 의견들을 취합하는 과정\n",
    "\n",
    "feed-forward-neural-network : 단어들의 정보 벡터를 통해 출력한다. (이는 다음 인코더 층에 입력으로 사용된다.)\n",
    "\n",
    "이 2개의 신경망 사이의 **잔차연결&정규화 층**: 이 층에서 잔차연결과 레이어 정규화가 이루어진다.\n",
    "잔차연결 : self-attention 과정을 거치면서 원래 embedding vector의 정보가 지나치게 손실 혹은 변형되는 것을 막는 역할\n",
    "multi-attention을 통과한 출력값과 본래 embedding vector가 결합하면서 본래 embeddig vector의 정보가 어느정도 유지될 수 있도록 한다.\n",
    "정규화 : layer의 출력 데이터에 대해 평균과 분산을 이용하여 정규화를 수행함으로써 데이터를 안정화하고 학습속도를 개선한다.\n",
    "\n",
    "#### Encoder의 self-attention 원리\n",
    "\n",
    "셀프 어텐션은 query, key, value인 3개의 벡터로 이루어져 있다.\n",
    "이 벡터들은 입력-단어의 임베딩 벡터로부터 각각의 가중치 행렬로 계산되어 나온다.\n",
    "\n",
    "query :  어텐션을 받는 단어(현재 주목을 받는 단어)가 어텐션과 고나련해 다른 단어들에게 던지는 질문 .. ?\n",
    "예시. 나에게 영향을 미치는 단어들 말해주세요 !\n",
    "key : 질문에 대한 대답\n",
    "예시. 제가 이정도로 당신에게 영향을 미칩니다.\n",
    "value : 어텐션을 받는 단어가 다른 단어에게 영향을 받는 정도 (내적을 통해 자신에게 오는 영향의 정도를 계산)\n",
    "\n",
    "정보량을 다 더함으로써 어텐션을 받는 단어에 대한 Attention vector가 오나성된다.\n",
    "나에게 가장 영향을 많이 주는 단어는 나 자신이지만, 자신 외에 나에게 영향을 미치는 단어들의 정보가 포함되면서 문장의 의미적인 구조인 문맥을 파악할 수 있다.\n",
    "\n",
    "#### Decoder의 동작 원리\n",
    "\n",
    "Decoder는 출력 단어 시퀀스의 첫 토큰 <start>로 시작하여 단어가 순차적으로 추가되는 시퀀스를 입력으로 받는다.\n",
    "\n",
    "Decoder는 Encoder와 달리 **mask multi-head attention**, 잔차연결&정규화의 2단계를 수행한 이후,\n",
    "인코더로부터 연결되는 Encoder-Decoder Attention, 잔차연결&정규화의 2단계가 추가된다.\n",
    "\n",
    "1 . **mask multi-head attention**\n",
    "\n",
    "Decoder의 특성상 self-attention이 뒤에서 앞으로 갈 수 없기 때문에 만들어진 매커니즘이다. (하나의 단어 예측 → 다음 단어 예측)\n",
    "첫번째 단어를 예측한 뒤, 다음 단어를 예측하기 때문에 첫째 단어에서 둘째 단어로만 영향을 미친다. (미래에서 영향을 받는 것은 당연히 어렵다.)\n",
    "이와 같이 Decoder에서 순방향으로만 Attention이 향하는 것을 구현한 매커니즘이다.\n",
    "\n",
    "2 . Encoder and Decoder Multihead Attention\n",
    "\n",
    "Encoder에서 Decoder로 향하는 어텐션을 구현한 층이다.\n",
    "query를 던지는 단어는 디코더에서 생성하고 있는 단어이다. (영어에서 한국어로 번역을 하고 있다면, 디코더에서 생성한 한글 단어가 query를 던지고, 여기에 반응(key, value)은 영어 단어로부터 온다.)\n",
    "생성되고 있는 한국어 단어에 대한 어텐션 정보는 인코더에 있는 영어 단어로부터 와야 한다.\n",
    "feed-forward-neural-network와 잔차연결&정규화는 encoder와 동일하게 수행된다.\n",
    "마지막 단계에서 선형변환과 softmax 층은 다음 단어를 예측하기 위해 사용된다.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. BERT의 이해와 활용\n",
    "\n",
    "BERT(Bidirectional Encoder Representations form Trnasformers)는 현재 시점에서 텍스트 마이닝 딥러닝 모형 중 가장 중요한 모형이다.\n",
    "텍스트 마이닝의 전분야에서 우수한 성능을 보여주고 있다.\n",
    "\n",
    "- 언어 모델과 학습법\n",
    "- BERT의 구조\n",
    "- 기본 활용 방법"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 언어 모델의 장점\n",
    "\n",
    "비지도 학습이 가능한 것\n",
    "\n",
    "트랜스포머로 번역 모델을 학습한다고 하면, 반드시 번역의 대상이 되는 문장과 번역한 문장이 쌍으로 존재해야한다. (답이 무조건 존재해야 한다.)\n",
    "언어모델은 주어진 단어의 시퀀스 뒤에 오는 단어를 예측하는 방식으로 학습하므로, 문장에 대한 답을 별도로 필요하지 않고 문서를 적절하게 잘라서\n",
    "학습이 가능하다.\n",
    "\n",
    "→ 우리는 사전에 언어모델을 이용해 미리 학습된 모델을 사용한다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 사전학습 언어모델의 이론적 이해\n",
    "\n",
    "언어모델 : 단어의 시퀀스에 대해 확률을 할당하는 모델\n",
    "주어진 입력 단어 시퀀스(입력 문장)에 대해 대상으로 하는 **출력 단어 시퀀스(출력 문장/ 하나의 단어 값 혹은 단어의 시퀀스)의 확률을 추정하는 것**으로\n",
    "하나의 단어가 아닌 단어 시퀀스, 즉 문장에 대해 확률을 할당하는 언어모델이 된다.\n",
    "\n",
    "언어모델은 자연어문장을 이용해 주어진 단어 시퀀스에 대해 다음 단어를 예측하도록 하는 데이터셋을 자동으로 생성함으로써 학습을 수행한다.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bert의 구조\n",
    "\n",
    "Bert(Bidirectional Encoder Representations from Transformers)는 언어 모델 기반의 학습을 이용해 언어에 대한 이해를 높이는 데 목적이 있다.\n",
    "Transformer의 앞 부분인 Encoder만 사용한 모형으로, 이 Encoder는 양방향(Bidirectional) self-attention을 구현하고 있다.\n",
    "\n",
    "#### GPT VS BERT\n",
    "GPT는 Transformer의 Decoder를 사용하여 문장 생성에 특화된 모델이다. 따라서 Encoder에서 Decoder로의 Attention은 생략되어 있고, self-attention의 순방향만 적용된다.\n",
    "반면, Bert는 Transformer의 Encoder 부분만 떼어서 사용을 하였는데, 양방향 self-attention을 모두 활용할 수 있다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 언어모델을 이용한 사전학습과 미세조정 학습\n",
    "\n",
    "BERT의 학습은 사전학습과 미세조정학습의 두 단계로 나누어진다.\n",
    "\n",
    "사전학습 : 언어에 대한 이해를 높이기 위한 비지도학습\n",
    "\n",
    "Q. Bert는 어떻게 학습을 수행하는 것인가?\n",
    "\n",
    "첫번째 방법 1. masking이라는 기법을 사용하여 모델을 학습시킨다. 즉, 단어를 가리고 가린 단어를 예측하게 한다.\n",
    "즉 순서와 관계없이, 문장 안에서 **랜덤한 위치의 단어를 지우고** 모형이 이 단어들을 예측하도록 한다.\n",
    "가려진 단어는 문장의 중간에 위치하며, 양쪽에 단어들이 있어 양방향 셀프 어텐션을 모두 이용해 예측하는 것이 가능하다.\n",
    "\n",
    "두번째 방법 2. 두 문장을 다루어 두 문장이 같은 의미인지 혹은 앞 문장의 내용으로 뒤 문장의 질문에 대한 답이 가능한지 등을 알아본다.\n",
    "Transformer는 Encoder와 Decoder로 구성되어 있어, 두 문장을 다루어야 하는 경우에 하나는 인코더로, 하나는 디코더로 보내는 것이지만,\n",
    "Bert는 Encoder밖에 없어 두 문장을 구분하는 토큰을 정의하고 두 문장 사이에 넣어서 하나의 시퀀스를 만든 후에 Encoder에서 한 번에 처리한다.\n",
    "\n",
    "미세조정학습 : 실제 수행하고자 하는 작업에 대한 지도학습\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 실습\n",
    "\n",
    "<Transformer로 수행할 수 있는 작업>\n",
    "\n",
    "transformers 라이브러리를 사용해 파이프라인을 이용하면 된다.\n",
    "\n",
    "1. 감정분석\n",
    "2. 문서분류\n",
    "3. 질의응답\n",
    "4. 문서생성\n",
    "5. 기계번역\n",
    "6. 문서요약\n",
    "\n",
    "이래의 파이프라인에서 사용한 모형 : https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eac155a1828540a4bda7bf23d7484854"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ahyeo\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "891f77d98eb945919033915ac9422a72"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "edabbc9563da48e497a7cc15aa735a20"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "153316b08ce846d1ba732d5588bfffdd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#감정 분석 결과 : POSITIVE, 감정 스코어 : 0.9999\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "clf = pipeline(\"sentiment-analysis\")\n",
    "result = clf(\"What a beautiful day!\")[0]\n",
    "print(\"#감정 분석 결과 : %s, 감정 스코어 : %0.4f\"%(result['label'], result['score']))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d093e22504c44d6f8baf7fac5c52bcf3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ahyeo\\.cache\\huggingface\\hub\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c0bcb42ebd674796b899c78a3a810c10"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65920b4060a5452291aa3ce75b3b4b7f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a28533bca3ad441ca8780557cf26d97b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "510c00c3b8b64fb691f783f1aa894965"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a1a6eff43774dc5a6c0ccf5d9df5cfb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank,  he thought. He went back to the hotel, but did not get into trouble before she turned back around. She was very friendly, friendly with the guests,\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text_generator = pipeline('text-generation')\n",
    "result = text_generator(\"Alice was beginning to get very tired of sitting by her sister on the bank, \")\n",
    "print(result[0]['generated_text'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dbf6e76ce58b4eb4ad34c37f65187085"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no: 29%\n",
      "yes: 71%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# 토크나이저와 모델 로드\n",
    "# BERT의 모형과 학습 ㅇ ㅝㄴ리에 최적화된 토큰화 방식을 사용한다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "\n",
    "# 입력 문장과 타겟 시퀀스\n",
    "input_sentence = \"She angered me with her inappropriate comments, rumor-spreading, and disrespectfulness at the formal dinner table\"\n",
    "target_sequence = \"She made me angry when she was rude at dinner\" #input_sentence의 요약 버전\n",
    "\n",
    "# 토큰화\n",
    "tokens = tokenizer(input_sentence, target_sequence, return_tensors=\"tf\")\n",
    "\n",
    "# 모델을 통한 예측 (logits는 softmax를 적용하기 이전의 신경망 결과)\n",
    "logits = model(**tokens).logits\n",
    "\n",
    "# Softmax를 사용하여 로짓을 확률로 변환 (확률값을 구하기 위해서는 softmax 적용 필수)\n",
    "results = tf.nn.softmax(logits, axis=-1).numpy()[0]\n",
    "\n",
    "# 결과 출력\n",
    "for i, label in enumerate(['no', 'yes']):\n",
    "    print(f\"{label}: {int(round(results[i] * 100))}%\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no: 95%\n",
      "yes: 5%\n"
     ]
    }
   ],
   "source": [
    "# 상관없는 문장으로 다시 한 번 수행\n",
    "target_sequence = \"The boy quickly ran across the finish line, seizing yet another victory\"\n",
    "tokens = tokenizer(input_sentence, target_sequence, return_tensors='tf')\n",
    "\n",
    "logits = model(**tokens).logits\n",
    "\n",
    "results = tf.nn.softmax(logits, axis=-1).numpy()[0]\n",
    "\n",
    "# 결과 출력\n",
    "for i, label in enumerate(['no', 'yes']):\n",
    "    print(f\"{label}: {int(round(results[i] * 100))}%\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 영화리뷰 데이터셋 사용"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\ahyeo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set count :  1600\n",
      "#Test set count :  400\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "nltk.download(\"movie_reviews\")\n",
    "\n",
    "fileids = movie_reviews.fileids()\n",
    "\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in fileids]\n",
    "categories = [movie_reviews.categories(fileid)[0] for fileid in fileids]\n",
    "\n",
    "label_dict = {'pos' : 0, 'neg' : 1}\n",
    "y = np.array([label_dict[c] for c in categories])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, y, test_size=0.2, random_state=7)\n",
    "\n",
    "print(\"#Train set count : \", len(X_train))\n",
    "print(\"#Test set count : \", len(X_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ee64f067ddf44cceba968b2dc27a1855"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ahyeo\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "efff2791f5104891a9ba4128d60cbaf9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8bcb838981c4b8dad73fea7d9c9cbe9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d3adf474c744c74a7c0eb877c1cc170"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK 영화리뷰 감성분석 정확도: 0.1575\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "if tf.config.experimental.list_physical_devices('GPU'):\n",
    "    device = '/GPU:0'\n",
    "else:\n",
    "    device = '/CPU:0'\n",
    "\n",
    "#BERT 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "batch_size = 10 #모형으로 한 번에 예측할 데이터의 수 (메모리를 많이 잡아먹어서 이렇게 수행)\n",
    "y_pred = [] #예측 결과 저장\n",
    "\n",
    "num_batch = len(y_test) // batch_size\n",
    "\n",
    "for i in range(num_batch):\n",
    "    inputs = tokenizer(\n",
    "        X_test[i * batch_size:(i + 1) * batch_size],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "    logits = model(**inputs).logits\n",
    "    pred = tf.nn.softmax(logits, axis=-1)\n",
    "    results = tf.argmax(pred, axis=1).numpy()\n",
    "    y_pred.extend(results.tolist())\n",
    "\n",
    "# 메모리 관리를 위해 TensorFlow 세션 클리어\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "score = np.sum(y_test == np.array(y_pred)) / len(y_test)\n",
    "print(\"NLTK 영화리뷰 감성분석 정확도:\", score) #왜 정확도가... 15.75%..?? -> 원래 85.75%가 나옴.."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BERT 사전학습 모형에 대한 미세조정 학습\n",
    "\n",
    "1 . 마스킹에 의한 빈 단어의 예측\n",
    "2 . 두 문장의 선후관계 예측\n",
    "\n",
    "< 사용되는 3개의 Embedding >\n",
    "\n",
    "1 . Token Embedding\n",
    "토큰 임베딩에는 문장에 사용된 단어 외에 두 종류의 특수 토큰이 추가된다. [CLS] 토큰과 [SEP] 토큰\n",
    "[CLS] 토큰 : 한 문서에 대한 문서 분류나 두 문서의 관계에 대한 분류를 하기 위한 정보를 수집해 최종적으로 출력하는 역할\n",
    "[SEP] 토큰 : seperator 토큰으로 한 문장의 끝을 나타내거나 두 문장을 분리한다.\n",
    "\n",
    "2 . Segement Embedding\n",
    "BERT에서 두 문장을 하나의 시퀀스로 만들어 입력으로 사용하므로, 각 토큰이 어느 문장에 속하는지 별도로 임베딩한다.\n",
    "\n",
    "3 . Position Embedding\n",
    "시퀀스에서의 순서를 나타낸다.\n",
    "\n",
    "\n",
    "BertTokenizer : 가장 기본적인 클래스\n",
    "bert-base-uncased : 미세조정학습 없이 언어모델에 대한 사전학습만 수행한 모형\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "59580cd9b7e14e52b13695ce793e64fc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ahyeo\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7643ce6ef105486fb9590be4cfe51b01"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa32858df6b7428c89cca710e90b7d27"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "80d15bfdf56945b4bfde4eb5c83a2d6f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a beautiful day! 토큰화 결과 ['what', 'a', 'beautiful', 'day', '!']\n",
      "Nvidia Titan XP has 12GB of VRAM 토큰화 결과 ['n', '##vid', '##ia', 'titan', 'xp', 'has', '12', '##gb', 'of', 'vr', '##am']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "sentence1 = \"What a beautiful day!\"\n",
    "sentence2 = \"Nvidia Titan XP has 12GB of VRAM\"\n",
    "\n",
    "print(sentence1, '토큰화 결과', tokenizer.tokenize(sentence1))\n",
    "print(sentence2, '토큰화 결과', tokenizer.tokenize(sentence2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%ㄹ\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert 입력 :  {'input_ids': [[101, 2054, 1037, 3376, 2154, 999, 102, 0, 0, 0, 0, 0, 0], [101, 1050, 17258, 2401, 16537, 26726, 2038, 2260, 18259, 1997, 27830, 3286, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer([sentence1, sentence2], padding = True)\n",
    "print(\"Bert 입력 : \", inputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "두 문장 시퀀스에 대한 BERT 입력 :  {'input_ids': [101, 2054, 1037, 3376, 2154, 999, 102, 1050, 17258, 2401, 16537, 26726, 2038, 2260, 18259, 1997, 27830, 3286, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(sentence1, sentence2, padding = True)\n",
    "print(\"두 문장 시퀀스에 대한 BERT 입력 : \", inputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\ahyeo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set count:  1600\n",
      "Test set count:  400\n"
     ]
    }
   ],
   "source": [
    "## 미세조정학습\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.model_selection import train_test_split #sklearn에서 제공하는 split 함수를 사용\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "fileids = movie_reviews.fileids() #movie review data에서 file id를 가져옴\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in fileids] #file id를 이용해 raw text file을 가져옴\n",
    "categories = [movie_reviews.categories(fileid)[0] for fileid in fileids]\n",
    "\n",
    "# label을 0, 1의 값으로 변환\n",
    "label_dict = {'pos':1, 'neg':0}\n",
    "y = [label_dict[c] for c in categories]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, y, test_size=0.2, random_state=7)\n",
    "\n",
    "print('Train set count: ', len(X_train))\n",
    "print('Test set count: ', len(X_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aaa866a8b96c45f7a062056f5f72b768"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ahyeo\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_input = tokenizer(X_train, truncation=True, padding=True, return_tensors=\"tf\")\n",
    "test_input = tokenizer(X_test, truncation=True, padding=True, return_tensors=\"tf\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_dataset(inputs, labels):\n",
    "    # 입력과 레이블을 텐서로 변환\n",
    "    inputs_tensors = {key: tf.convert_to_tensor(val) for key, val in inputs.items()}\n",
    "    labels_tensors = tf.convert_to_tensor(labels)\n",
    "\n",
    "    # 텐서로부터 데이터셋 생성\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs_tensors, labels_tensors))\n",
    "    return dataset\n",
    "\n",
    "# 훈련 및 테스트 데이터셋 생성\n",
    "train_dataset = create_dataset(train_input, y_train)\n",
    "test_dataset = create_dataset(test_input, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining\\lib\\site-packages\\datasets\\load.py:753: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(predictions, labels):\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nTrainer requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFTrainer\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 10\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Trainer, TrainingArguments\n\u001B[0;32m      3\u001B[0m training_args \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[0;32m      4\u001B[0m     output_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./results\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;66;03m#체크포인트 출력 폴더\u001B[39;00m\n\u001B[0;32m      5\u001B[0m     num_train_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m, \u001B[38;5;66;03m#학습 수\u001B[39;00m\n\u001B[0;32m      6\u001B[0m     per_device_train_batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m8\u001B[39m, \u001B[38;5;66;03m#학습에서 사용할 배치의 크기 (BERT는 메모리를 많이 차지하기 때문에 배치의 사이즈가 매우 적다.)\u001B[39;00m\n\u001B[0;32m      7\u001B[0m     per_device_eval_batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m \u001B[38;5;66;03m#예측에서 사용할 배치의 크기\u001B[39;00m\n\u001B[0;32m      8\u001B[0m )\n\u001B[1;32m---> 10\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[43mTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m#학습할 모형\u001B[39;49;00m\n\u001B[0;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtraining_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m#위에서 정의한 학습 매개변수들\u001B[39;49;00m\n\u001B[0;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m#학습데이터셋\u001B[39;49;00m\n\u001B[0;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompute_metrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompute_metrics\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m#검증 방식\u001B[39;49;00m\n\u001B[0;32m     15\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m#미세조정학습 실행\u001B[39;00m\n\u001B[0;32m     18\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\transformers\\utils\\dummy_pt_objects.py:9610\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   9609\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m-> 9610\u001B[0m     \u001B[43mrequires_backends\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtorch\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\transformers\\utils\\import_utils.py:1309\u001B[0m, in \u001B[0;36mrequires_backends\u001B[1;34m(obj, backends)\u001B[0m\n\u001B[0;32m   1307\u001B[0m \u001B[38;5;66;03m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001B[39;00m\n\u001B[0;32m   1308\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m backends \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m backends \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_available() \u001B[38;5;129;01mand\u001B[39;00m is_tf_available():\n\u001B[1;32m-> 1309\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001B[38;5;241m.\u001B[39mformat(name))\n\u001B[0;32m   1311\u001B[0m \u001B[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001B[39;00m\n\u001B[0;32m   1312\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m backends \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m backends \u001B[38;5;129;01mand\u001B[39;00m is_torch_available() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_tf_available():\n",
      "\u001B[1;31mImportError\u001B[0m: \nTrainer requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFTrainer\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ],
   "source": [
    "#이건 PyTorch 기반임..\n",
    "# from datasets import load_metric\n",
    "#\n",
    "# metric = load_metric(\"accuracy\")\n",
    "#\n",
    "# def compute_metrics(predictions, labels):\n",
    "#     predictions = np.argmax(predictions, axis=-1)\n",
    "#     return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir = './results', #체크포인트 출력 폴더\n",
    "#     num_train_epochs = 2, #학습 수\n",
    "#     per_device_train_batch_size = 8, #학습에서 사용할 배치의 크기 (BERT는 메모리를 많이 차지하기 때문에 배치의 사이즈가 매우 적다.)\n",
    "#     per_device_eval_batch_size=16 #예측에서 사용할 배치의 크기\n",
    "# )\n",
    "#\n",
    "# trainer = Trainer(\n",
    "#     model = model, #학습할 모형\n",
    "#     args = training_args, #위에서 정의한 학습 매개변수들\n",
    "#     train_dataset = train_dataset, #학습데이터셋\n",
    "#     compute_metrics=compute_metrics #검증 방식\n",
    "# )\n",
    "#\n",
    "# #미세조정학습 실행\n",
    "# trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "  4/200 [..............................] - ETA: 1:49:53 - loss: 0.8092 - accuracy: 0.5000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 17\u001B[0m\n\u001B[0;32m     14\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Eval Metrics:\u001B[39m\u001B[38;5;124m\"\u001B[39m, eval_metrics, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# 모델 학습\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshuffle\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[43m          \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[43m          \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_dataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[43m          \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mCustomCallback\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1161\u001B[0m, in \u001B[0;36mTFPreTrainedModel.fit\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1158\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(keras\u001B[38;5;241m.\u001B[39mModel\u001B[38;5;241m.\u001B[39mfit)\n\u001B[0;32m   1159\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m   1160\u001B[0m     args, kwargs \u001B[38;5;241m=\u001B[39m convert_batch_encoding(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m-> 1161\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1734\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[0;32m   1735\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1736\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1739\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m   1740\u001B[0m ):\n\u001B[0;32m   1741\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[1;32m-> 1742\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1743\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m   1744\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    822\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    824\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 825\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    827\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    828\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    854\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    855\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[0;32m    856\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[1;32m--> 857\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_no_variable_creation_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[0;32m    858\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    859\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[0;32m    860\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[0;32m    861\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001B[0m, in \u001B[0;36mTracingCompiler.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    145\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    146\u001B[0m   (concrete_function,\n\u001B[0;32m    147\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[1;32m--> 148\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconcrete_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    149\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconcrete_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, args, captured_inputs)\u001B[0m\n\u001B[0;32m   1345\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1346\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1347\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1348\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1349\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1350\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1351\u001B[0m     args,\n\u001B[0;32m   1352\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1353\u001B[0m     executing_eagerly)\n\u001B[0;32m   1354\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001B[0m, in \u001B[0;36mAtomicFunction.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m record\u001B[38;5;241m.\u001B[39mstop_recording():\n\u001B[0;32m    195\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mexecuting_eagerly():\n\u001B[1;32m--> 196\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_bound_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    197\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    198\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction_type\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflat_outputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    200\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    201\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    202\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m make_call_op_in_graph(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mlist\u001B[39m(args))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001B[0m, in \u001B[0;36mContext.call_function\u001B[1;34m(self, name, tensor_inputs, num_outputs)\u001B[0m\n\u001B[0;32m   1455\u001B[0m cancellation_context \u001B[38;5;241m=\u001B[39m cancellation\u001B[38;5;241m.\u001B[39mcontext()\n\u001B[0;32m   1456\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cancellation_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1457\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1458\u001B[0m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1459\u001B[0m \u001B[43m      \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1460\u001B[0m \u001B[43m      \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1461\u001B[0m \u001B[43m      \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1462\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1463\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1464\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1465\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m   1466\u001B[0m       name\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   1467\u001B[0m       num_outputs\u001B[38;5;241m=\u001B[39mnum_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1471\u001B[0m       cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_context,\n\u001B[0;32m   1472\u001B[0m   )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     52\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 53\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     54\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     56\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#tensorflow 기반으로 수정\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 커스텀 콜백 정의\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # 검증 데이터셋에 대한 예측\n",
    "        predictions = self.model.predict(test_dataset.map(lambda x, y: x))\n",
    "        # compute_metrics 함수를 사용하여 메트릭 계산\n",
    "        eval_metrics = compute_metrics(predictions, np.concatenate([y for x, y in test_dataset], axis=0))\n",
    "        print(f\"\\nEpoch {epoch+1} Eval Metrics:\", eval_metrics, \"\\n\")\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(train_dataset.shuffle(1000).batch(8),\n",
    "          epochs=2,\n",
    "          validation_data=test_dataset.batch(16),\n",
    "          callbacks=[CustomCallback()])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = model.evaluate(test_dataset)\n",
    "print(\"test loss, test acc:\", results)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "GPU가 없어서 시간 관계상 epoch 멈춤."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
