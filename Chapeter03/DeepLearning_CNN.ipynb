{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## CNN\n",
    "\n",
    "원래 이미지 인식에 탁월한 효과를 보이는 딥러닝 신경망이지만, 문서 분류에 뛰어난 효과를 보이는 것으로 입증되어 텍스트 마이닝 분야에서도\n",
    "활발하게 쓰이는 모형이 되었다.\n",
    "BERT의 결과와 CNN을 결합한 모형이 제안되는 등 여전히 많이 사용된다.\n",
    "\n",
    "CNN : 2차원 행렬로부터 주변 정보를 요약해 이미지를 분류할 수 있는 특성들을 추출하는 딥러닝 기법이다. (컬러 이미지 : 3차원 행렬)\n",
    "\n",
    "**CNN 구조**\n",
    "1 . 컨볼루션 단계\n",
    "원본 이미지에 필터를 적용해 주변 정보를 요약하는 단계\n",
    "필터는 원본 이미지보다 작은 2차원 행렬로 학습의 대상이 되는 파라미터 (파라미터에 따라 요약정보는 달라짐. weight의 느낌)\n",
    "이 단계에서 중요한 요소는 필터 혹은 채너의 수이다.\n",
    "필터를 2개를 사용한다면, 두 개의 요약된 2차원 행렬을 만들어낼 수 있다. (하나의 이미지 -> 2가지의 다른 정보 추출)\n",
    "2 . 풀링 단계\n",
    "요약된 2차원 행렬을 축소하는 단계\n",
    "max pooling의 경우 각 필터 범위 안에 있는 가장 높은 값을 추출해내 새로운 2차원 행렬을 만든다. (이미지 너비 축소)\n",
    "\n",
    "이 특성을 통해 이미지의 정보를 축략한 특성들을 만들어내고, 모형의 마지막 단계에서 이 특성을 가지고 이미지를 판별한다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CNN을 이용한 문서 분류\n",
    "\n",
    "텍스트 마이닝에 딥러닝을 적용하는 이유는 **문맥 파악**이며, CNN을 사용하는 이유는 단어들의 연속된 나열에 대해 앞 뒤 단어들 간의 주변정보를 요약해낼 수 있다는 것이다.\n",
    "\n",
    "문서에서는 단어들의 나열을 보기 때문에 **2차원이 아닌 1차원 방향으로만** 필터를 이동시키면서 컨볼루션을 수행한다.\n",
    "(옆/아래로 이동이 아닌 아래로만 이동)\n",
    "\n",
    "문서는 한 단어가 하나의 행으로 반환되어 단어들은 순서에 따라 **아래방향**으로 나열된다.\n",
    "여기서 단어들 간의 주변 정보를 요약하기 위해서 필터의 열 수를 단어 벡터의 수와 같게 하고(단어의 개수 -> dictionary에 있는 단어의 전체 개수/ 혹은 밀집벡터)\n",
    "단어의 나열 방향에 따라 필터를 **아래**로만 이동하며 주변 정보를 요약한다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[0;32m      9\u001B[0m fileids \u001B[38;5;241m=\u001B[39m movie_reviews\u001B[38;5;241m.\u001B[39mfileids()\n\u001B[1;32m---> 10\u001B[0m reviews \u001B[38;5;241m=\u001B[39m [movie_reviews\u001B[38;5;241m.\u001B[39mraw(fileids) \u001B[38;5;28;01mfor\u001B[39;00m fileid \u001B[38;5;129;01min\u001B[39;00m fileids]\n\u001B[0;32m     11\u001B[0m categories \u001B[38;5;241m=\u001B[39m [movie_reviews\u001B[38;5;241m.\u001B[39mcategories(fileid)[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m fileid \u001B[38;5;129;01min\u001B[39;00m fileids]\n\u001B[0;32m     13\u001B[0m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mseed(\u001B[38;5;241m7\u001B[39m)\n",
      "Cell \u001B[1;32mIn[4], line 10\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[0;32m      9\u001B[0m fileids \u001B[38;5;241m=\u001B[39m movie_reviews\u001B[38;5;241m.\u001B[39mfileids()\n\u001B[1;32m---> 10\u001B[0m reviews \u001B[38;5;241m=\u001B[39m [\u001B[43mmovie_reviews\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfileids\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m fileid \u001B[38;5;129;01min\u001B[39;00m fileids]\n\u001B[0;32m     11\u001B[0m categories \u001B[38;5;241m=\u001B[39m [movie_reviews\u001B[38;5;241m.\u001B[39mcategories(fileid)[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m fileid \u001B[38;5;129;01min\u001B[39;00m fileids]\n\u001B[0;32m     13\u001B[0m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mseed(\u001B[38;5;241m7\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:403\u001B[0m, in \u001B[0;36mCategorizedCorpusReader.raw\u001B[1;34m(self, fileids, categories)\u001B[0m\n\u001B[0;32m    402\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mraw\u001B[39m(\u001B[38;5;28mself\u001B[39m, fileids\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, categories\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 403\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_resolve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfileids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcategories\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:219\u001B[0m, in \u001B[0;36mCorpusReader.raw\u001B[1;34m(self, fileids)\u001B[0m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m fileids:\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mopen(f) \u001B[38;5;28;01mas\u001B[39;00m fp:\n\u001B[1;32m--> 219\u001B[0m         contents\u001B[38;5;241m.\u001B[39mappend(fp\u001B[38;5;241m.\u001B[39mread())\n\u001B[0;32m    220\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m concat(contents)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\nltk\\data.py:1167\u001B[0m, in \u001B[0;36mSeekableUnicodeStreamReader.__exit__\u001B[1;34m(self, type, value, traceback)\u001B[0m\n\u001B[0;32m   1166\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__exit__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mtype\u001B[39m, value, traceback):\n\u001B[1;32m-> 1167\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclose\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TextMining\\lib\\site-packages\\nltk\\data.py:1196\u001B[0m, in \u001B[0;36mSeekableUnicodeStreamReader.close\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mclose\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   1193\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1194\u001B[0m \u001B[38;5;124;03m    Close the underlying stream.\u001B[39;00m\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1196\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclose\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "### CNN을 이용한 NLTK 영화 리뷰\n",
    "from nltk.corpus import movie_reviews\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fileids = movie_reviews.fileids()\n",
    "reviews = [movie_reviews.raw(fileids) for fileid in fileids]\n",
    "categories = [movie_reviews.categories(fileid)[0] for fileid in fileids]\n",
    "\n",
    "np.random.seed(7)\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "max_words = 1000\n",
    "maxlen = 500\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_words, oov_token = \"UNK\")\n",
    "tokenizer.fit_on_texts(reviews) #단어 인덱스 구축\n",
    "\n",
    "X = tokenizer.texts_to_sequences(reviews) #만들어진 단어 인덱스를 이용해 변환\n",
    "X= pad_sequences(X, maxlen = maxlen, truncating = 'pre')\n",
    "\n",
    "label_dict = {'pos':0, 'neg':1}\n",
    "y = np.array([label_dict[c] for c in categories])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential([\n",
    "    #word embedding layer\n",
    "    Embedding(max_words, 64, input_length = maxlen),   #500X64의 행렬로 변환 (500은 maxlen)\n",
    "    Conv1D(128, #채널의 수 128개\n",
    "           5, #필터의 크기 5x5\n",
    "           padding = 'valid', #패딩 없음\n",
    "           activation = 'relu',\n",
    "           strides = 1), #필터 이동 간격 : 1\n",
    "    MaxPooling1D(),\n",
    "    Conv1D(256, #채널의 수를 256개로 늘림\n",
    "           5,\n",
    "           padding = 'valid',\n",
    "           activation = 'relu',\n",
    "           strides = 1),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate = 1e-3)\n",
    "model.compile(optimizer = adam, loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs = 20,\n",
    "                    batch_size = 256,\n",
    "                    verbose = 0,\n",
    "                    validation_split = 0.2)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_results(history, metric):\n",
    "    plt.plot(history.history[metric], 'b', label = 'Training ' + metric)\n",
    "    plt.plot(history.history['val_'+metric], 'r--', label = 'validation '+ metric)\n",
    "    plt.title('Training. vs Validation. '+metric)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_results(history, 'loss')\n",
    "\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy : {score[1]:.3f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 결과 분석\n",
    "500개나 되는 단어들로 문서가 이루어져 있을 때, 그 500개의 단어들을 다 연결해 문맥을 파악하는 것이 더 효율적인지, 아니면 5개의 단어 정도로 끊어가며 좁은 범위에서 문맥을 파악하고 그 결과를 합치는 것이 더 효율적인지는 알 수 없다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
