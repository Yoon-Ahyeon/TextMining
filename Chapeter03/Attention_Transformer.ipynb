{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Attention And Transformer\n",
    "\n",
    "### Seq2seq\n",
    "입력으로 일련의 단어들이 들어오고 이를 이용해서 다시 일련의 단어들을 생성해야 하는 문제를 말한다.\n",
    "**과정**\n",
    "문장에 대한 내용을 이해 → 정해진 형태(벡터)로 이해된 내용 저장 → 번역의 첫 단어 예측(사전에 있는 단어들에 대한 확률 계산) → 높은 확률의 단어 제안\n",
    "→ 벡터와 첫 단어를 통해 다음 단어 예측 → 반복\n",
    "\n",
    "**구조**\n",
    "Encoder and Decoder\n",
    "예를 들어 번역으로 보자면, (영어 → 한국어)\n",
    "Encoder : 영어 문장을 이해하는 역할 (번역하고자 하는 문장)\n",
    "\n",
    "은닉층을 통해 앞 단어로부터 순차적으로 정보가 축적되고 마지막 <end> 입력을 받은 은닉층의 노드는 영어 문장 전체의 문맥 정보를 내포한다.\n",
    "→ **마지막 노드**에 전체적인 정보가 들어가 있다.\n",
    "\n",
    "Decoder : 이 문맥정보로부터 한국어 문장을 생성하는 역할을 한다.\n",
    "<start> : 문장의 시작 혹은 번역의 시작을 알리는 벡터\n",
    "디코더는 한 번에 실행되지 않고 각 단어를 예측하는 단계가 **순차적으로** 실행된다.\n",
    "<end> : 번역을 마친다.\n",
    "\n",
    "결론 : Encoder가 문맥정보를 축적하면, Decoder는 이 문맥정보에 자신이 생성한 단어들을 하나씩 결합하면서 다음 단어를 예측하게 되고,\n",
    "생성해야 할 단어의 수만큼 모형을 이용한 예측이 반복된다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Attention을 이용한 성능 향상\n",
    "\n",
    "#### seq2seq의 문제점\n",
    "문맥 정보가 인코더의 마지막 벡터 하나에 집중되는 현상이 나타난다.\n",
    "이 하나의 벡터에 번역할 무장의 모든 문맥 정보가 포함되고, 이를 이용해 디코더에서 반복적으로 다음 단어를 예측한다.\n",
    "만약, 번역해야할 문장이 엄청 길다면, 하나의 벡터에 많은 정보가 잘 축적되고 또 이를 디코더에서 풀어내기가 어렵다. (성능이 떨어짐)\n",
    "\n",
    "#### Attention\n",
    "하나의 단어가 단어를 번역할 때 직접 관여할 수 있도록 한다.\n",
    "Encoder의 윗부분에 **context vector**를 생성한다.\n",
    "이 벡터는 첫 단어의 예측에 가장 많은 영향을 미치는 단어에 대한 정보가 담겨있다.\n",
    "단어별로 **Attention Score**를 계산하고 softmax를 적용한 가중치를 표현한다. (가중치가 가장 높은 단어가 현재 예측해야할 단어에 가장 영향을 많이 미치는 것)\n",
    "이 단어는 학습을 통해 결정된다.\n",
    "\n",
    "결론 : 단어들의 임베딩 벡터에 대해 가중치를 반영해 가중 합계를 구하면, context vector가 되고 이 context vector가 encoder의 마지막 벡터와 deoder의 입력값과 합쳐져 첫 단어를 생성한다. **context vecotr는 당연히 예측할 단어의 순서에 따라 계속 변화한다.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Self-Attention\n",
    "\n",
    "예시. The animal didn't cross the street because it was too tired.\n",
    "VS The animal didn't cross the street because it was too wide.\n",
    "\n",
    "여기서 it이 가리키는 명사는 다르다. (animal - street)\n",
    "이와 같이 동일한 단어라도 **문맥에 따라 다른 의미를 갖게 되고** 그 의미에 영향을 미치는 단어가 문장에 존재한다.\n",
    "\n",
    "셀프 어텐션의 목적 : 문장 내에서 단어 간 영향을 표현하는 것\n",
    "어떤 단어를 벡터로 임베딩할 때, 그 단어에 영향을 미치는 다른 단어들의 정보를 **함께 인코딩**하는 것이다.\n",
    "(각 단어들에 대해 그 단어에 영향을 미치는 단어들의 정보를 선별해(가중치) 자신에게 축적한다.)\n",
    "→ 이렇게 되면, 어느 한 벡터가 전체 문맥에 대한 정보를 축적하고 있지 않다. (모두 자기 자신에 대한 벡터를 가지고 있음)\n",
    "Encoding 과정에서 문맥에 대한 정보는 각 단어에 골고루 분포하고 Decoding 과정에서는 입력 문장의 **모든 단어의 임베딩 벡터**를 어텐션 형태로 사용한다.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transformer\n",
    "\n",
    "Transformer는 기존 seq2seq 모형에서 RNN 혹은 LSTM 구조를 다 버리고 오직 Attention에만 의지한 모형을 제안하였다.\n",
    "\n",
    "#### Transformer 구조\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
