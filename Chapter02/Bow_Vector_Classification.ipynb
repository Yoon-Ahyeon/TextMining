{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Bow 기반의 문서 분류\n",
    "\n",
    "주어진 문서를 미리 정의된 클래스로 분류하는 작업\n",
    "\n",
    "<머신러닝의 분류 작업>\n",
    "\n",
    "1. 로지스틱 회귀분석\n",
    "2. 결정트리\n",
    "3. 나이브 베이즈 **(가장 많이 사용)**\n",
    "4. SVM\n",
    "\n",
    "※ 주의사항\n",
    "\n",
    "뉴스의 분야가 라벨로 달려 있어야 한다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set size :  2034\n",
      "#Test set size :  1353\n",
      "#Selected categories :  ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
      "Train labels :  {0, 1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "newsgroup_train = fetch_20newsgroups(subset = 'train', remove = ('headers', 'footers',\n",
    "                                                                 'quotes'),\n",
    "                                     categories = categories)\n",
    "newsgroup_test = fetch_20newsgroups(subset = 'test', remove = ('headers', 'footers',\n",
    "                                                                 'quotes'),\n",
    "                                     categories = categories)\n",
    "\n",
    "print('#Train set size : ', len(newsgroup_train.data))\n",
    "print('#Test set size : ', len(newsgroup_test.data))\n",
    "print('#Selected categories : ', newsgroup_train.target_names)\n",
    "print('Train labels : ', set(newsgroup_train.target))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set text samples :  Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "#Train set label samples :  1\n",
      "#Test set text samples :  TRry the SKywatch project in  Arizona.\n",
      "#Test set label samples :  2\n"
     ]
    }
   ],
   "source": [
    "print(\"#Train set text samples : \", newsgroup_train.data[0])\n",
    "print(\"#Train set label samples : \", newsgroup_train.target[0])\n",
    "print(\"#Test set text samples : \", newsgroup_test.data[0])\n",
    "print(\"#Test set label samples : \", newsgroup_test.target[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 카운트 기반 특성 추출"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set dimension :  (2034, 2000)\n",
      "Test set dimension (1353, 2000)\n"
     ]
    }
   ],
   "source": [
    "X_train = newsgroup_train.data\n",
    "y_train = newsgroup_train.target\n",
    "\n",
    "X_test = newsgroup_test.data\n",
    "y_test = newsgroup_test.target\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# min_df : 단어가 최소 이 문서에 5개는 나타나야 한다. (문서에서 거의 쓰이지 않는다 - 예외)\n",
    "# max_df : 단어가 문서의 50%를 초과해 나타나는 단어를 제외한다.\n",
    "cv = CountVectorizer(max_features = 2000, min_df = 5, max_df = 0.5)\n",
    "\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "print('Train set dimension : ', X_train_cv.shape) # (train 문서의 수, 특성의 개수)\n",
    "X_test_cv = cv.transform(X_test)\n",
    "print('Test set dimension', X_test_cv.shape) # (test 문서의 수, 특성의 개수)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 : 0, 000 : 0, 01 : 0, 04 : 0, 05 : 0, 10 : 0, 100 : 0, 1000 : 0, 11 : 0, 12 : 0, 128 : 0, 129 : 0, 13 : 0, 130 : 0, 14 : 0, 15 : 0, 16 : 0, 17 : 0, 18 : 0, 19 : 0, 1987 : 0, 1988 : 0, 1989 : 0, 1990 : 0, 1991 : 0, 1992 : 0, 1993 : 0, 20 : 0, 200 : 0, 202 : 0, 21 : 0, 22 : 0, 23 : 0, 24 : 0, 25 : 0, 256 : 0, 26 : 0, 27 : 0, 28 : 0, 2d : 0, 30 : 0, 300 : 0, 31 : 0, 32 : 0, 33 : 0, 34 : 0, 35 : 0, 39 : 0, 3d : 0, 40 : 0, 400 : 0, 42 : 0, 45 : 0, 50 : 0, 500 : 0, 60 : 0, 600 : 0, 65 : 0, 70 : 0, 75 : 0, 80 : 0, 800 : 0, 90 : 0, 900 : 0, 91 : 0, 92 : 0, 93 : 0, 95 : 0, _the : 0, ability : 0, able : 1, abortion : 0, about : 1, above : 0, absolute : 0, absolutely : 0, ac : 0, accept : 0, acceptable : 0, accepted : 0, access : 0, according : 0, account : 0, accurate : 0, across : 0, act : 0, action : 0, actions : 0, active : 0, activities : 0, activity : 0, acts : 0, actual : 0, actually : 0, ad : 0, add : 0, added : 0, addition : 0, additional : 0, address : 0, "
     ]
    }
   ],
   "source": [
    "for word, count in zip(\n",
    "    # 사용된 단어의 이름과 빈도 수 추출\n",
    "    cv.get_feature_names_out()[:100], X_train_cv[0].toarray()[0, :100]\n",
    "):\n",
    "    print(word, ':', count, end = ', ')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. 나이브베이즈를 이용한 문서 분류\n",
    "\n",
    "새로운 기사에 나온 단어들이 각각 어떤 확률로 경제 관련 기사와 과학 기사에 나오는지 계산하고, 이를 잘 결합하여\n",
    "**새로운 기사가 어느 분야**에 속하는 지 확인한다.\n",
    "\n",
    "사전확률을 계산하여, 특성을 고려해 이를 더 정확한 확률로 바꾼다.\n",
    "\n",
    "Q. MultinomialNB\n",
    "이산적인 특성 값들을 이용해 분류하고자 할 때 사용한다.\n",
    "이산적이란, 연속적인 값이 아닌 값이라는 뜻으로 countvector가 이에 해당한다.\n",
    "\n",
    "매개변수\n",
    "alpha : 모델의 복잡도 조절 (이 값을 늘리면, 통계 데이터가 완만해지고 복잡도가 낮아진다.)\n",
    "TFidfVectorizer 사용 가능"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score : 0.824\n",
      "Test set score : 0.732\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "NB_clf = MultinomialNB()\n",
    "NB_clf.fit(X_train_cv, y_train)\n",
    "\n",
    "print('Train set score : {:.3f}'.format(NB_clf.score(X_train_cv, y_train)))\n",
    "print('Test set score : {:.3f}'.format(NB_clf.score(X_test_cv, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#First document and label in test data :  TRry the SKywatch project in  Arizona. 2\n",
      "#second document and label in test data :  The Vatican library recently made a tour of the US.\n",
      " Can anyone help me in finding a FTP site where this collection is \n",
      " available. 1\n",
      "#Predicted Labels :  [2 1]\n",
      "#Predicted categories :  sci.space comp.graphics\n"
     ]
    }
   ],
   "source": [
    "print('#First document and label in test data : ', X_test[0], y_test[0])\n",
    "print('#second document and label in test data : ', X_test[1], y_test[1])\n",
    "\n",
    "pred = NB_clf.predict(X_test_cv[:2])\n",
    "\n",
    "print('#Predicted Labels : ', pred)\n",
    "\n",
    "# 주제 (topic) 전달\n",
    "print(\n",
    "    '#Predicted categories : ',\n",
    "    newsgroup_train.target_names[pred[0]],\n",
    "    newsgroup_train.target_names[pred[1]]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score : 0.862\n",
      "Test set score : 0.741\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=2000, min_df = 5, max_df = 0.5)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "NB_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('Train set score : {:.3f}'. format(NB_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score : {:.3f}'. format(NB_clf.score(X_test_tfidf, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(NB_clf, 'coef_')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism : you,not,are,be,this,have,as,what,they,if\n",
      "comp.graphics : you,on,graphics,this,have,any,can,or,with,thanks\n",
      "sci.space : space,on,you,be,was,this,as,they,have,are\n",
      "talk.religion.misc : you,not,he,are,as,this,be,god,was,they\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def top10_features(classifier, vectorizer, categories):\n",
    "    # word 이름 추출\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names_out())\n",
    "    for i, category in enumerate(categories):\n",
    "        # 역순 정렬을 위해 계수에 음수를 취해 정렬 후, 앞에서 10개 추출\n",
    "        top10 = np.argsort(-classifier.feature_log_prob_[i])[:10]\n",
    "        # 카테고리와 영향이 큰 특성 10개 반환\n",
    "        print(\"%s : %s\" % (category, \",\".join(feature_names[top10])))\n",
    "\n",
    "top10_features(NB_clf, tfidf, newsgroup_train.target_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 로지스틱 회귀분석을 이용한 문서 분류\n",
    "\n",
    "회귀분석은 예측하고자 하는 값이 연속적일 때 사용하는 반면, 로지스틱 회귀 분석은 라벨이 연속적인 값이 아니고 분류에 해당될 때 사용한다.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score : 0.930\n",
      "Test set score : 0.734\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR_clf = LogisticRegression()\n",
    "\n",
    "LR_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('Train set score : {:.3f}'. format(LR_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score : {:.3f}'. format(LR_clf.score(X_test_tfidf, y_test))) # 과적합 발생 가능성 있음"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 과적합 해결 방법\n",
    "\n",
    "※ 릿지 회귀를 이용한 과적합 방지\n",
    "\n",
    "릿지 회귀는 회귀분석에 정규화를 사용하는 알고리즘으로, 최적화를 위한 목적함수에 정규화 항목을 넣어서 특성에 대한 계수가 지나치게 커지는 것을 억제한다.\n",
    "\n",
    "Ridge Classfier의 매개변수\n",
    "alpha : 정규화의 정도 조절 (값이 커질 수록, 정규화의 비중이 커져 계수를 더 많이 억제한다.)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score : 0.960\n",
      "Test set score : 0.735\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "ridge_clf = RidgeClassifier()\n",
    "\n",
    "ridge_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('Train set score : {:.3f}'. format(ridge_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score : {:.3f}'. format(ridge_clf.score(X_test_tfidf, y_test))) # 과적합 발생 가능성 있음"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 과적합 해결방법\n",
    "\n",
    "1 . validation set을 이용한 그리드 서치\n",
    "2 . 하이퍼파라미터 튜닝"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Alpha 1.600000 at max validation score 0.825553\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_ridge, X_val_ridge, y_train_ridge, y_val_ridge = train_test_split(X_train_tfidf, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "max_score = 0\n",
    "max_alpha = 0\n",
    "\n",
    "for alpha in np.arange(0.1, 10, 0.1):\n",
    "    ridge_clf = RidgeClassifier(alpha = alpha)\n",
    "    ridge_clf.fit(X_train_ridge, y_train_ridge)\n",
    "    score = ridge_clf.score(X_val_ridge, y_val_ridge)\n",
    "    if score > max_score :\n",
    "        max_score = score\n",
    "        max_alpha = alpha\n",
    "\n",
    "print('Max Alpha {:3f} at max validation score {:3f}'.format(max_alpha, max_score))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score : 0.947\n",
      "Test set score : 0.739\n"
     ]
    }
   ],
   "source": [
    "ridge_clf = RidgeClassifier(alpha = 1.6)\n",
    "ridge_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('Train set score : {:.3f}'. format(ridge_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score : {:.3f}'. format(ridge_clf.score(X_test_tfidf, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def top10_features(classifier, vectorizer, categories):\n",
    "    # word 이름 추출\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names_out())\n",
    "    for i, category in enumerate(categories):\n",
    "        # 역순 정렬을 위해 계수에 음수를 취해 정렬 후, 앞에서 10개 추출\n",
    "        top10 = np.argsort(-classifier.coef_[i])[:10]\n",
    "        # 카테고리와 영향이 큰 특성 10개 반환\n",
    "        print(\"%s : %s\" % (category, \",\".join(feature_names[top10])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism : bobby,religion,atheism,atheists,motto,punishment,islam,deletion,islamic,satan\n",
      "comp.graphics : graphics,computer,3d,file,image,hi,42,using,screen,looking\n",
      "sci.space : space,orbit,nasa,spacecraft,moon,sci,launch,flight,funding,idea\n",
      "talk.religion.misc : christian,christians,fbi,blood,order,jesus,objective,children,christ,hudson\n"
     ]
    }
   ],
   "source": [
    "top10_features(ridge_clf, tfidf, newsgroup_train.target_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 라쏘 회귀를 이용한 특성 선택\n",
    "\n",
    "특성의 계수에 대해 정규화를 한다.\n",
    "라쏘는 정규화를 할 때, 특성의 계수가 0에 가까워지면, 이를 완전히 0으로 바꾼다. (절댓값)\n",
    "어떤 특성의 계수가 0이라는 것은 그 특성은 분류에 전혀 영향을 미치지 않는다는 것이다.\n",
    "-> 특성의 수를 줄여주는 효과가 있다. (정확도가 정확히 향상된다고 보기는 어렵다.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score : 0.819\n",
      "Test set score : 0.724\n"
     ]
    }
   ],
   "source": [
    "lasso_clf = LogisticRegression(penalty='l1', solver = 'liblinear', C = 1) # C는 alpha의 역수\n",
    "\n",
    "lasso_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('Train set score : {:.3f}'. format(lasso_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score : {:.3f}'. format(lasso_clf.score(X_test_tfidf, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Used features count : 437 out of 2000\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    '#Used features count : {}'.format(np.sum(lasso_clf.coef_ != 0)), #특성 선택\n",
    "    'out of',\n",
    "    X_train_tfidf.shape[1]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism : bobby,atheism,atheists,islam,religion,islamic,motto,atheist,satan,vice\n",
      "comp.graphics : graphics,image,3d,file,computer,hi,video,files,looking,sphere\n",
      "sci.space : space,orbit,launch,nasa,spacecraft,flight,moon,dc,shuttle,solar\n",
      "talk.religion.misc : fbi,christian,christians,christ,order,jesus,children,objective,context,blood\n"
     ]
    }
   ],
   "source": [
    "top10_features(lasso_clf, tfidf, newsgroup_train.target_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 결정트리를 이용한 문서 분류 방법\n",
    "\n",
    "1. DecisionTree\n",
    "2. RandomForest\n",
    "3. GradientBoosting\n",
    "\n",
    "결정트리가 과적합되는 성향이 매우 강하다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score : 0.977 Test set score : 0.536\n",
      "Train set score : 0.977 Test set score : 0.685\n",
      "Train set score : 0.933 Test set score : 0.696\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=7)\n",
    "tree.fit(X_train_tfidf, y_train)\n",
    "print(\n",
    "    'Train set score : {:.3f}'. format(tree.score(X_train_tfidf, y_train)),\n",
    "    'Test set score : {:.3f}'. format(tree.score(X_test_tfidf, y_test))\n",
    ")\n",
    "\n",
    "forest = RandomForestClassifier(random_state=7)\n",
    "forest.fit(X_train_tfidf, y_train)\n",
    "print(\n",
    "    'Train set score : {:.3f}'. format(forest.score(X_train_tfidf, y_train)),\n",
    "    'Test set score : {:.3f}'. format(forest.score(X_test_tfidf, y_test))\n",
    ")\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=7)\n",
    "gb.fit(X_train_tfidf, y_train)\n",
    "print(\n",
    "    'Train set score : {:.3f}'. format(gb.score(X_train_tfidf, y_train)),\n",
    "    'Test set score : {:.3f}'. format(gb.score(X_test_tfidf, y_test))\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "space : 0.126, graphics : 0.080, atheism : 0.024, thanks : 0.023, file : 0.021, orbit : 0.020, jesus : 0.018, god : 0.018, hi : 0.017, nasa : 0.015, image : 0.015, files : 0.014, christ : 0.010, moon : 0.010, bobby : 0.010, launch : 0.010, looking : 0.010, christian : 0.010, atheists : 0.009, christians : 0.009, fbi : 0.009, 3d : 0.008, you : 0.008, not : 0.008, islamic : 0.007, religion : 0.007, spacecraft : 0.007, flight : 0.007, computer : 0.007, islam : 0.007, ftp : 0.006, color : 0.006, software : 0.005, atheist : 0.005, card : 0.005, people : 0.005, koresh : 0.005, his : 0.005, kent : 0.004, sphere : 0.004, "
     ]
    }
   ],
   "source": [
    "sorted_feature_importances = sorted(\n",
    "    zip(tfidf.get_feature_names_out(), gb.feature_importances_),\n",
    "    key = lambda x:x[1],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "for feature, value in sorted_feature_importances[:40]:\n",
    "    print('%s : %.3f' % (feature, value), end = ', ')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%ㅇ\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 성능 높이기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score : 0.930\n",
      "Test set score : 0.751\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words('english')\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "\n",
    "RegTok = RegexpTokenizer(\"[\\w']{3,}\")\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = RegTok.tokenize(text.lower())\n",
    "    words = [word for word in tokens if (word not in english_stops) and len(word) > 2]\n",
    "    features = (list(map(lambda token: PorterStemmer().stem(token), words)))\n",
    "    return features\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer = tokenizer, max_features= 2000, max_df = 0.5, min_df = 5)\n",
    "\n",
    "X_train_tfidf= tfidf.fit_transform(X_train)\n",
    "X_test_tfidf=  tfidf.transform(X_test)\n",
    "\n",
    "LR_clf = LogisticRegression()\n",
    "LR_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('Train set score : {:.3f}'. format(LR_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score : {:.3f}'. format(LR_clf.score(X_test_tfidf, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahyeo\\anaconda3\\envs\\TextMining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set Dimension :  (2034, 4056)\n",
      "Train set score : 0.953\n",
      "Test set score : 0.761\n"
     ]
    }
   ],
   "source": [
    "# 특성 추출기의 단어 제한을 없앤다\n",
    "tfidf = TfidfVectorizer(tokenizer = tokenizer, max_df = 0.5, min_df = 5)\n",
    "\n",
    "X_train_tfidf= tfidf.fit_transform(X_train)\n",
    "X_test_tfidf=  tfidf.transform(X_test)\n",
    "\n",
    "LR_clf = LogisticRegression()\n",
    "LR_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"#Train set Dimension : \", X_train_tfidf.shape)\n",
    "print('Train set score : {:.3f}'. format(LR_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score : {:.3f}'. format(LR_clf.score(X_test_tfidf, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 카운트 기반의 문제점과 N-gram을 이용한 보완"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
